{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import sys\n",
    "sys.path.append(\"../../../spatial-clust-scripts-main/\")\n",
    "import model\n",
    "import warnings\n",
    "import numpy as np\n",
    "import leidenalg\n",
    "import igraph as ig\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import defaultdict\n",
    "import graph\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import skimage\n",
    "# import custom functions\n",
    "import utils\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from validclust import dunn\n",
    "from sklearn.metrics import pairwise_distances\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## during testing only test snap_gnn\n",
    "\n",
    "class SNAP_GNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features=args.gnn_input_dim, out_features=args.fc_dim)\n",
    "        self.cnn_fc = nn.Linear(in_features=args.cnn_input_dim, out_features=args.cnn_dim)\n",
    "        self.feat_conv1 = GCNConv(args.fc_dim, args.latent_dim)\n",
    "        self.feat_conv2 = GCNConv(args.latent_dim, args.fc_out_dim)\n",
    "        \n",
    "        self.spat_conv1 = GCNConv(args.cnn_dim, args.cnn_latent_dim)\n",
    "        self.spat_conv2 = GCNConv(args.cnn_latent_dim, args.cnn_out_dim)\n",
    "        \n",
    "        self.proj1 = nn.Linear(in_features=args.fc_out_dim+args.cnn_out_dim, \n",
    "                              out_features=args.hid_out_dim)\n",
    "        self.proj2 = nn.Linear(in_features=args.hid_out_dim, \n",
    "                              out_features=args.out_dim)\n",
    "\n",
    "    def feat_gnn_encoder(self, feat, feat_edge_index):\n",
    "        feat = F.relu(self.fc(feat))\n",
    "        feat = F.relu(self.feat_conv1(feat, feat_edge_index))\n",
    "        feat = self.feat_conv2(feat, feat_edge_index)\n",
    "        \n",
    "        return feat\n",
    "    \n",
    "    def spat_gnn_encoder(self, spat, spat_edge_index):\n",
    "        spat = F.relu(self.cnn_fc(spat))\n",
    "        spat = F.relu(self.spat_conv1(spat, spat_edge_index))\n",
    "        spat = self.spat_conv2(spat, spat_edge_index)\n",
    "        \n",
    "        return spat\n",
    "    \n",
    "    def encoder(self, feat, spat, feat_edge_index, spat_edge_index):\n",
    "        x_feat = self.feat_gnn_encoder(feat, feat_edge_index)\n",
    "        x_spat = self.spat_gnn_encoder(spat, spat_edge_index)\n",
    "        x = torch.cat((x_feat, x_spat), dim = 1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def forward(self, feat, spat, feat_edge_index, spat_edge_index):\n",
    "        x = F.relu(self.encoder(feat, spat, feat_edge_index, spat_edge_index))\n",
    "        x = self.proj1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.proj2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    gnn_input_dim = 31\n",
    "    cnn_input_dim = 128\n",
    "    fc_dim = latent_dim = 32\n",
    "    cnn_dim = cnn_latent_dim = 32\n",
    "   # out_dim = 14 * 2 # dont know this value yet\n",
    "    fc_out_dim = 33\n",
    "    cnn_out_dim = 11\n",
    "    hid_out_dim = 33\n",
    "\n",
    "    criterion = \"L1\"\n",
    "    learning_rate = 1e-3\n",
    "    epochs = 10000\n",
    "    print_every = 1000\n",
    "    average_iter = 100\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gnn_embed(model, cell_nbhd, feat, spat, feat_edge_index, spat_edge_index, verbose=False):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    cell_nbhd = cell_nbhd.to(args.device)\n",
    "    model = model.to(args.device)\n",
    "    if args.criterion == \"L1\":\n",
    "        print(\"Use L1 Loss\")\n",
    "        criterion = nn.L1Loss()\n",
    "    elif args.criterion == \"L2\":\n",
    "        print(\"Use L2 Loss\")\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        print(\"Cross Entropy\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loss_epoch = []\n",
    "    #criterion = nn.L1Loss()\n",
    "    for e in range(1, 1+args.epochs):\n",
    "        model.train()\n",
    "        predicted_nbhd = model(features, cnn_embedding, feat_edge_index, spat_edge_index)\n",
    "        # Compute prediction error\n",
    "        loss = criterion(predicted_nbhd, cell_nbhd)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # take one step\n",
    "        optimizer.step()\n",
    "\n",
    "        # record the loss\n",
    "        curr_train_loss = loss.item()\n",
    "        if verbose and e % args.print_every  == 0:\n",
    "            print(f'===Epoch {e}, the training loss is {curr_train_loss:>0.8f}==', flush=True)\n",
    "        train_loss_epoch.append(curr_train_loss)\n",
    "    return model.encoder(feat, spat, feat_edge_index, spat_edge_index).cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04641036==\n",
      "===Epoch 2000, the training loss is 0.04187335==\n",
      "===Epoch 3000, the training loss is 0.03774583==\n",
      "===Epoch 4000, the training loss is 0.03665170==\n",
      "===Epoch 5000, the training loss is 0.03200557==\n",
      "===Epoch 6000, the training loss is 0.03121771==\n",
      "===Epoch 7000, the training loss is 0.02906567==\n",
      "===Epoch 8000, the training loss is 0.02819327==\n",
      "===Epoch 9000, the training loss is 0.02777823==\n",
      "===Epoch 10000, the training loss is 0.02751600==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04255724==\n",
      "===Epoch 2000, the training loss is 0.04069012==\n",
      "===Epoch 3000, the training loss is 0.04010318==\n",
      "===Epoch 4000, the training loss is 0.03778130==\n",
      "===Epoch 5000, the training loss is 0.03619723==\n",
      "===Epoch 6000, the training loss is 0.03567549==\n",
      "===Epoch 7000, the training loss is 0.03537280==\n",
      "===Epoch 8000, the training loss is 0.03499094==\n",
      "===Epoch 9000, the training loss is 0.03484614==\n",
      "===Epoch 10000, the training loss is 0.03471065==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04981270==\n",
      "===Epoch 2000, the training loss is 0.04844642==\n",
      "===Epoch 3000, the training loss is 0.04056495==\n",
      "===Epoch 4000, the training loss is 0.03910176==\n",
      "===Epoch 5000, the training loss is 0.03806001==\n",
      "===Epoch 6000, the training loss is 0.03727837==\n",
      "===Epoch 7000, the training loss is 0.03706819==\n",
      "===Epoch 8000, the training loss is 0.03689457==\n",
      "===Epoch 9000, the training loss is 0.03650222==\n",
      "===Epoch 10000, the training loss is 0.03217190==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04970918==\n",
      "===Epoch 2000, the training loss is 0.04765928==\n",
      "===Epoch 3000, the training loss is 0.04429138==\n",
      "===Epoch 4000, the training loss is 0.04329575==\n",
      "===Epoch 5000, the training loss is 0.04309972==\n",
      "===Epoch 6000, the training loss is 0.04291733==\n",
      "===Epoch 7000, the training loss is 0.04279971==\n",
      "===Epoch 8000, the training loss is 0.04266689==\n",
      "===Epoch 9000, the training loss is 0.04238386==\n",
      "===Epoch 10000, the training loss is 0.04190808==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.05306917==\n",
      "===Epoch 2000, the training loss is 0.05255474==\n",
      "===Epoch 3000, the training loss is 0.05175558==\n",
      "===Epoch 4000, the training loss is 0.05150239==\n",
      "===Epoch 5000, the training loss is 0.05050926==\n",
      "===Epoch 6000, the training loss is 0.05036593==\n",
      "===Epoch 7000, the training loss is 0.04948323==\n",
      "===Epoch 8000, the training loss is 0.04890613==\n",
      "===Epoch 9000, the training loss is 0.04873030==\n",
      "===Epoch 10000, the training loss is 0.04787347==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04291672==\n",
      "===Epoch 2000, the training loss is 0.04113796==\n",
      "===Epoch 3000, the training loss is 0.03631607==\n",
      "===Epoch 4000, the training loss is 0.03533773==\n",
      "===Epoch 5000, the training loss is 0.03494232==\n",
      "===Epoch 6000, the training loss is 0.03451372==\n",
      "===Epoch 7000, the training loss is 0.03424257==\n",
      "===Epoch 8000, the training loss is 0.03414914==\n",
      "===Epoch 9000, the training loss is 0.03408937==\n",
      "===Epoch 10000, the training loss is 0.03399601==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04940643==\n",
      "===Epoch 2000, the training loss is 0.04282705==\n",
      "===Epoch 3000, the training loss is 0.03914718==\n",
      "===Epoch 4000, the training loss is 0.03444909==\n",
      "===Epoch 5000, the training loss is 0.03380190==\n",
      "===Epoch 6000, the training loss is 0.03348765==\n",
      "===Epoch 7000, the training loss is 0.03330298==\n",
      "===Epoch 8000, the training loss is 0.03304487==\n",
      "===Epoch 9000, the training loss is 0.03283862==\n",
      "===Epoch 10000, the training loss is 0.03271162==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04458611==\n",
      "===Epoch 2000, the training loss is 0.03462189==\n",
      "===Epoch 3000, the training loss is 0.03310642==\n",
      "===Epoch 4000, the training loss is 0.03118763==\n",
      "===Epoch 5000, the training loss is 0.03032437==\n",
      "===Epoch 6000, the training loss is 0.02984628==\n",
      "===Epoch 7000, the training loss is 0.02944829==\n",
      "===Epoch 8000, the training loss is 0.02753456==\n",
      "===Epoch 9000, the training loss is 0.02712845==\n",
      "===Epoch 10000, the training loss is 0.02679441==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04728142==\n",
      "===Epoch 2000, the training loss is 0.04016212==\n",
      "===Epoch 3000, the training loss is 0.03707329==\n",
      "===Epoch 4000, the training loss is 0.03616014==\n",
      "===Epoch 5000, the training loss is 0.03548424==\n",
      "===Epoch 6000, the training loss is 0.03188718==\n",
      "===Epoch 7000, the training loss is 0.03038592==\n",
      "===Epoch 8000, the training loss is 0.02997792==\n",
      "===Epoch 9000, the training loss is 0.02642675==\n",
      "===Epoch 10000, the training loss is 0.02622974==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04555871==\n",
      "===Epoch 2000, the training loss is 0.04476434==\n",
      "===Epoch 3000, the training loss is 0.04318458==\n",
      "===Epoch 4000, the training loss is 0.04289250==\n",
      "===Epoch 5000, the training loss is 0.04271993==\n",
      "===Epoch 6000, the training loss is 0.04233155==\n",
      "===Epoch 7000, the training loss is 0.04188503==\n",
      "===Epoch 8000, the training loss is 0.04159017==\n",
      "===Epoch 9000, the training loss is 0.04143403==\n",
      "===Epoch 10000, the training loss is 0.04133982==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.05157320==\n",
      "===Epoch 2000, the training loss is 0.04816975==\n",
      "===Epoch 3000, the training loss is 0.04448121==\n",
      "===Epoch 4000, the training loss is 0.04358762==\n",
      "===Epoch 5000, the training loss is 0.04241132==\n",
      "===Epoch 6000, the training loss is 0.04212421==\n",
      "===Epoch 7000, the training loss is 0.04185434==\n",
      "===Epoch 8000, the training loss is 0.04162478==\n",
      "===Epoch 9000, the training loss is 0.04145354==\n",
      "===Epoch 10000, the training loss is 0.04080335==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04978769==\n",
      "===Epoch 2000, the training loss is 0.04333773==\n",
      "===Epoch 3000, the training loss is 0.04199626==\n",
      "===Epoch 4000, the training loss is 0.04138112==\n",
      "===Epoch 5000, the training loss is 0.04068369==\n",
      "===Epoch 6000, the training loss is 0.03842175==\n",
      "===Epoch 7000, the training loss is 0.03815233==\n",
      "===Epoch 8000, the training loss is 0.03802631==\n",
      "===Epoch 9000, the training loss is 0.03778518==\n",
      "===Epoch 10000, the training loss is 0.03769279==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04892620==\n",
      "===Epoch 2000, the training loss is 0.04343219==\n",
      "===Epoch 3000, the training loss is 0.04219256==\n",
      "===Epoch 4000, the training loss is 0.04172924==\n",
      "===Epoch 5000, the training loss is 0.04157339==\n",
      "===Epoch 6000, the training loss is 0.04122722==\n",
      "===Epoch 7000, the training loss is 0.03679384==\n",
      "===Epoch 8000, the training loss is 0.03643704==\n",
      "===Epoch 9000, the training loss is 0.03613065==\n",
      "===Epoch 10000, the training loss is 0.03545314==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.05022875==\n",
      "===Epoch 2000, the training loss is 0.04579285==\n",
      "===Epoch 3000, the training loss is 0.04110982==\n",
      "===Epoch 4000, the training loss is 0.04064789==\n",
      "===Epoch 5000, the training loss is 0.03966085==\n",
      "===Epoch 6000, the training loss is 0.03891350==\n",
      "===Epoch 7000, the training loss is 0.03184279==\n",
      "===Epoch 8000, the training loss is 0.03128863==\n",
      "===Epoch 9000, the training loss is 0.03101122==\n",
      "===Epoch 10000, the training loss is 0.03084946==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04988808==\n",
      "===Epoch 2000, the training loss is 0.04435540==\n",
      "===Epoch 3000, the training loss is 0.03620351==\n",
      "===Epoch 4000, the training loss is 0.03491926==\n",
      "===Epoch 5000, the training loss is 0.03454413==\n",
      "===Epoch 6000, the training loss is 0.03372492==\n",
      "===Epoch 7000, the training loss is 0.03296283==\n",
      "===Epoch 8000, the training loss is 0.03240400==\n",
      "===Epoch 9000, the training loss is 0.03218766==\n",
      "===Epoch 10000, the training loss is 0.03163039==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04686114==\n",
      "===Epoch 2000, the training loss is 0.04471438==\n",
      "===Epoch 3000, the training loss is 0.03941783==\n",
      "===Epoch 4000, the training loss is 0.03909577==\n",
      "===Epoch 5000, the training loss is 0.03875722==\n",
      "===Epoch 6000, the training loss is 0.03732800==\n",
      "===Epoch 7000, the training loss is 0.03681414==\n",
      "===Epoch 8000, the training loss is 0.03658273==\n",
      "===Epoch 9000, the training loss is 0.03645117==\n",
      "===Epoch 10000, the training loss is 0.03628486==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.05004159==\n",
      "===Epoch 2000, the training loss is 0.04063626==\n",
      "===Epoch 3000, the training loss is 0.03915640==\n",
      "===Epoch 4000, the training loss is 0.03889237==\n",
      "===Epoch 5000, the training loss is 0.03835201==\n",
      "===Epoch 6000, the training loss is 0.03795516==\n",
      "===Epoch 7000, the training loss is 0.03773050==\n",
      "===Epoch 8000, the training loss is 0.03679617==\n",
      "===Epoch 9000, the training loss is 0.03659539==\n",
      "===Epoch 10000, the training loss is 0.03646535==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04985050==\n",
      "===Epoch 2000, the training loss is 0.04868467==\n",
      "===Epoch 3000, the training loss is 0.04823920==\n",
      "===Epoch 4000, the training loss is 0.04765506==\n",
      "===Epoch 5000, the training loss is 0.04726898==\n",
      "===Epoch 6000, the training loss is 0.04703045==\n",
      "===Epoch 7000, the training loss is 0.04615031==\n",
      "===Epoch 8000, the training loss is 0.04593431==\n",
      "===Epoch 9000, the training loss is 0.04596518==\n",
      "===Epoch 10000, the training loss is 0.04578266==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04742057==\n",
      "===Epoch 2000, the training loss is 0.04559378==\n",
      "===Epoch 3000, the training loss is 0.04517581==\n",
      "===Epoch 4000, the training loss is 0.04482087==\n",
      "===Epoch 5000, the training loss is 0.04407271==\n",
      "===Epoch 6000, the training loss is 0.04388849==\n",
      "===Epoch 7000, the training loss is 0.04356229==\n",
      "===Epoch 8000, the training loss is 0.04308161==\n",
      "===Epoch 9000, the training loss is 0.04298814==\n",
      "===Epoch 10000, the training loss is 0.04293817==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04354138==\n",
      "===Epoch 2000, the training loss is 0.04146321==\n",
      "===Epoch 3000, the training loss is 0.04081815==\n",
      "===Epoch 4000, the training loss is 0.03699220==\n",
      "===Epoch 5000, the training loss is 0.03647314==\n",
      "===Epoch 6000, the training loss is 0.03615342==\n",
      "===Epoch 7000, the training loss is 0.03582533==\n",
      "===Epoch 8000, the training loss is 0.03548712==\n",
      "===Epoch 9000, the training loss is 0.03518673==\n",
      "===Epoch 10000, the training loss is 0.03505614==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [53:04<3:32:19, 3184.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04831894==\n",
      "===Epoch 2000, the training loss is 0.04213379==\n",
      "===Epoch 3000, the training loss is 0.03735451==\n",
      "===Epoch 4000, the training loss is 0.03590268==\n",
      "===Epoch 5000, the training loss is 0.03509637==\n",
      "===Epoch 6000, the training loss is 0.03419801==\n",
      "===Epoch 7000, the training loss is 0.03379675==\n",
      "===Epoch 8000, the training loss is 0.03360881==\n",
      "===Epoch 9000, the training loss is 0.03292587==\n",
      "===Epoch 10000, the training loss is 0.03256495==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.05012828==\n",
      "===Epoch 2000, the training loss is 0.04865364==\n",
      "===Epoch 3000, the training loss is 0.04823461==\n",
      "===Epoch 4000, the training loss is 0.04811414==\n",
      "===Epoch 5000, the training loss is 0.04719884==\n",
      "===Epoch 6000, the training loss is 0.04677669==\n",
      "===Epoch 7000, the training loss is 0.04661282==\n",
      "===Epoch 8000, the training loss is 0.04602120==\n",
      "===Epoch 9000, the training loss is 0.04525156==\n",
      "===Epoch 10000, the training loss is 0.04452870==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04693502==\n",
      "===Epoch 2000, the training loss is 0.03554684==\n",
      "===Epoch 3000, the training loss is 0.03380739==\n",
      "===Epoch 4000, the training loss is 0.03296347==\n",
      "===Epoch 5000, the training loss is 0.03238319==\n",
      "===Epoch 6000, the training loss is 0.02938101==\n",
      "===Epoch 7000, the training loss is 0.02879063==\n",
      "===Epoch 8000, the training loss is 0.02876593==\n",
      "===Epoch 9000, the training loss is 0.02767499==\n",
      "===Epoch 10000, the training loss is 0.02732140==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04411348==\n",
      "===Epoch 2000, the training loss is 0.03811740==\n",
      "===Epoch 3000, the training loss is 0.03682958==\n",
      "===Epoch 4000, the training loss is 0.03586661==\n",
      "===Epoch 5000, the training loss is 0.03537405==\n",
      "===Epoch 6000, the training loss is 0.03508377==\n",
      "===Epoch 7000, the training loss is 0.03486451==\n",
      "===Epoch 8000, the training loss is 0.03475526==\n",
      "===Epoch 9000, the training loss is 0.03464129==\n",
      "===Epoch 10000, the training loss is 0.03195392==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04841832==\n",
      "===Epoch 2000, the training loss is 0.04527707==\n",
      "===Epoch 3000, the training loss is 0.04303296==\n",
      "===Epoch 4000, the training loss is 0.04202095==\n",
      "===Epoch 5000, the training loss is 0.04178144==\n",
      "===Epoch 6000, the training loss is 0.04158844==\n",
      "===Epoch 7000, the training loss is 0.04143957==\n",
      "===Epoch 8000, the training loss is 0.04106591==\n",
      "===Epoch 9000, the training loss is 0.04084476==\n",
      "===Epoch 10000, the training loss is 0.04072989==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04337782==\n",
      "===Epoch 2000, the training loss is 0.03763775==\n",
      "===Epoch 3000, the training loss is 0.03684217==\n",
      "===Epoch 4000, the training loss is 0.03310620==\n",
      "===Epoch 5000, the training loss is 0.03243303==\n",
      "===Epoch 6000, the training loss is 0.03206321==\n",
      "===Epoch 7000, the training loss is 0.03188332==\n",
      "===Epoch 8000, the training loss is 0.03171221==\n",
      "===Epoch 9000, the training loss is 0.02905105==\n",
      "===Epoch 10000, the training loss is 0.02842785==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04972644==\n",
      "===Epoch 2000, the training loss is 0.04229574==\n",
      "===Epoch 3000, the training loss is 0.03696321==\n",
      "===Epoch 4000, the training loss is 0.03588248==\n",
      "===Epoch 5000, the training loss is 0.03545798==\n",
      "===Epoch 6000, the training loss is 0.03506233==\n",
      "===Epoch 7000, the training loss is 0.03461784==\n",
      "===Epoch 8000, the training loss is 0.03435860==\n",
      "===Epoch 9000, the training loss is 0.03418899==\n",
      "===Epoch 10000, the training loss is 0.03406081==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04202820==\n",
      "===Epoch 2000, the training loss is 0.03950449==\n",
      "===Epoch 3000, the training loss is 0.03857609==\n",
      "===Epoch 4000, the training loss is 0.03788967==\n",
      "===Epoch 5000, the training loss is 0.03784547==\n",
      "===Epoch 6000, the training loss is 0.03767707==\n",
      "===Epoch 7000, the training loss is 0.03752623==\n",
      "===Epoch 8000, the training loss is 0.03744408==\n",
      "===Epoch 9000, the training loss is 0.03727117==\n",
      "===Epoch 10000, the training loss is 0.03398614==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04753871==\n",
      "===Epoch 2000, the training loss is 0.04197118==\n",
      "===Epoch 3000, the training loss is 0.04059349==\n",
      "===Epoch 4000, the training loss is 0.03917094==\n",
      "===Epoch 5000, the training loss is 0.03858608==\n",
      "===Epoch 6000, the training loss is 0.03838748==\n",
      "===Epoch 7000, the training loss is 0.03813991==\n",
      "===Epoch 8000, the training loss is 0.03801765==\n",
      "===Epoch 9000, the training loss is 0.03775828==\n",
      "===Epoch 10000, the training loss is 0.03771388==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04601979==\n",
      "===Epoch 2000, the training loss is 0.04454213==\n",
      "===Epoch 3000, the training loss is 0.04152564==\n",
      "===Epoch 4000, the training loss is 0.04095783==\n",
      "===Epoch 5000, the training loss is 0.04055775==\n",
      "===Epoch 6000, the training loss is 0.04040411==\n",
      "===Epoch 7000, the training loss is 0.03757331==\n",
      "===Epoch 8000, the training loss is 0.03646145==\n",
      "===Epoch 9000, the training loss is 0.03598188==\n",
      "===Epoch 10000, the training loss is 0.03528733==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04782508==\n",
      "===Epoch 2000, the training loss is 0.04140714==\n",
      "===Epoch 3000, the training loss is 0.03989543==\n",
      "===Epoch 4000, the training loss is 0.03951499==\n",
      "===Epoch 5000, the training loss is 0.03937091==\n",
      "===Epoch 6000, the training loss is 0.03915608==\n",
      "===Epoch 7000, the training loss is 0.03709462==\n",
      "===Epoch 8000, the training loss is 0.03606413==\n",
      "===Epoch 9000, the training loss is 0.03405638==\n",
      "===Epoch 10000, the training loss is 0.03390850==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04326731==\n",
      "===Epoch 2000, the training loss is 0.03593550==\n",
      "===Epoch 3000, the training loss is 0.03390671==\n",
      "===Epoch 4000, the training loss is 0.03311848==\n",
      "===Epoch 5000, the training loss is 0.03288462==\n",
      "===Epoch 6000, the training loss is 0.03245475==\n",
      "===Epoch 7000, the training loss is 0.03227371==\n",
      "===Epoch 8000, the training loss is 0.03200018==\n",
      "===Epoch 9000, the training loss is 0.03167979==\n",
      "===Epoch 10000, the training loss is 0.03136690==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04290159==\n",
      "===Epoch 2000, the training loss is 0.03866170==\n",
      "===Epoch 3000, the training loss is 0.03751756==\n",
      "===Epoch 4000, the training loss is 0.03698164==\n",
      "===Epoch 5000, the training loss is 0.03674259==\n",
      "===Epoch 6000, the training loss is 0.03603244==\n",
      "===Epoch 7000, the training loss is 0.03280120==\n",
      "===Epoch 8000, the training loss is 0.03236724==\n",
      "===Epoch 9000, the training loss is 0.03216359==\n",
      "===Epoch 10000, the training loss is 0.03209966==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04427146==\n",
      "===Epoch 2000, the training loss is 0.03558654==\n",
      "===Epoch 3000, the training loss is 0.03452892==\n",
      "===Epoch 4000, the training loss is 0.03266855==\n",
      "===Epoch 5000, the training loss is 0.03208155==\n",
      "===Epoch 6000, the training loss is 0.03014770==\n",
      "===Epoch 7000, the training loss is 0.02939708==\n",
      "===Epoch 8000, the training loss is 0.02926469==\n",
      "===Epoch 9000, the training loss is 0.02843588==\n",
      "===Epoch 10000, the training loss is 0.02827102==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04763217==\n",
      "===Epoch 2000, the training loss is 0.04563415==\n",
      "===Epoch 3000, the training loss is 0.04498135==\n",
      "===Epoch 4000, the training loss is 0.04384133==\n",
      "===Epoch 5000, the training loss is 0.04337650==\n",
      "===Epoch 6000, the training loss is 0.04322741==\n",
      "===Epoch 7000, the training loss is 0.04301574==\n",
      "===Epoch 8000, the training loss is 0.03991733==\n",
      "===Epoch 9000, the training loss is 0.03974263==\n",
      "===Epoch 10000, the training loss is 0.03960944==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03969078==\n",
      "===Epoch 2000, the training loss is 0.03674452==\n",
      "===Epoch 3000, the training loss is 0.03315518==\n",
      "===Epoch 4000, the training loss is 0.03237658==\n",
      "===Epoch 5000, the training loss is 0.03207688==\n",
      "===Epoch 6000, the training loss is 0.03177287==\n",
      "===Epoch 7000, the training loss is 0.03157361==\n",
      "===Epoch 8000, the training loss is 0.03134117==\n",
      "===Epoch 9000, the training loss is 0.03113292==\n",
      "===Epoch 10000, the training loss is 0.03093381==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04959920==\n",
      "===Epoch 2000, the training loss is 0.04522177==\n",
      "===Epoch 3000, the training loss is 0.04354442==\n",
      "===Epoch 4000, the training loss is 0.04282338==\n",
      "===Epoch 5000, the training loss is 0.04191590==\n",
      "===Epoch 6000, the training loss is 0.04141156==\n",
      "===Epoch 7000, the training loss is 0.04011088==\n",
      "===Epoch 8000, the training loss is 0.03783343==\n",
      "===Epoch 9000, the training loss is 0.03746802==\n",
      "===Epoch 10000, the training loss is 0.03613006==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04565327==\n",
      "===Epoch 2000, the training loss is 0.04414359==\n",
      "===Epoch 3000, the training loss is 0.04046521==\n",
      "===Epoch 4000, the training loss is 0.03570117==\n",
      "===Epoch 5000, the training loss is 0.03483144==\n",
      "===Epoch 6000, the training loss is 0.03057067==\n",
      "===Epoch 7000, the training loss is 0.03018527==\n",
      "===Epoch 8000, the training loss is 0.02994898==\n",
      "===Epoch 9000, the training loss is 0.02975519==\n",
      "===Epoch 10000, the training loss is 0.02961094==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04657208==\n",
      "===Epoch 2000, the training loss is 0.03795537==\n",
      "===Epoch 3000, the training loss is 0.03699040==\n",
      "===Epoch 4000, the training loss is 0.03640066==\n",
      "===Epoch 5000, the training loss is 0.03633759==\n",
      "===Epoch 6000, the training loss is 0.03607031==\n",
      "===Epoch 7000, the training loss is 0.03580676==\n",
      "===Epoch 8000, the training loss is 0.03568135==\n",
      "===Epoch 9000, the training loss is 0.03523814==\n",
      "===Epoch 10000, the training loss is 0.03495756==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04603811==\n",
      "===Epoch 2000, the training loss is 0.04370115==\n",
      "===Epoch 3000, the training loss is 0.04091977==\n",
      "===Epoch 4000, the training loss is 0.03961611==\n",
      "===Epoch 5000, the training loss is 0.03930242==\n",
      "===Epoch 6000, the training loss is 0.03901639==\n",
      "===Epoch 7000, the training loss is 0.03884698==\n",
      "===Epoch 8000, the training loss is 0.03511495==\n",
      "===Epoch 9000, the training loss is 0.03454170==\n",
      "===Epoch 10000, the training loss is 0.03440943==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [2:24:27<3:46:50, 4536.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04989804==\n",
      "===Epoch 2000, the training loss is 0.04867017==\n",
      "===Epoch 3000, the training loss is 0.04807844==\n",
      "===Epoch 4000, the training loss is 0.04791469==\n",
      "===Epoch 5000, the training loss is 0.04424809==\n",
      "===Epoch 6000, the training loss is 0.04330412==\n",
      "===Epoch 7000, the training loss is 0.04295341==\n",
      "===Epoch 8000, the training loss is 0.04257382==\n",
      "===Epoch 9000, the training loss is 0.04201119==\n",
      "===Epoch 10000, the training loss is 0.04147965==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04517568==\n",
      "===Epoch 2000, the training loss is 0.04339271==\n",
      "===Epoch 3000, the training loss is 0.04227398==\n",
      "===Epoch 4000, the training loss is 0.03922925==\n",
      "===Epoch 5000, the training loss is 0.03852629==\n",
      "===Epoch 6000, the training loss is 0.03830757==\n",
      "===Epoch 7000, the training loss is 0.03513542==\n",
      "===Epoch 8000, the training loss is 0.03458884==\n",
      "===Epoch 9000, the training loss is 0.03436426==\n",
      "===Epoch 10000, the training loss is 0.03401122==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04148729==\n",
      "===Epoch 2000, the training loss is 0.03751056==\n",
      "===Epoch 3000, the training loss is 0.03618442==\n",
      "===Epoch 4000, the training loss is 0.03541055==\n",
      "===Epoch 5000, the training loss is 0.03502158==\n",
      "===Epoch 6000, the training loss is 0.03436589==\n",
      "===Epoch 7000, the training loss is 0.03418797==\n",
      "===Epoch 8000, the training loss is 0.03359289==\n",
      "===Epoch 9000, the training loss is 0.02870081==\n",
      "===Epoch 10000, the training loss is 0.02825783==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04976121==\n",
      "===Epoch 2000, the training loss is 0.04890471==\n",
      "===Epoch 3000, the training loss is 0.04815458==\n",
      "===Epoch 4000, the training loss is 0.04745515==\n",
      "===Epoch 5000, the training loss is 0.04722205==\n",
      "===Epoch 6000, the training loss is 0.04520901==\n",
      "===Epoch 7000, the training loss is 0.04450134==\n",
      "===Epoch 8000, the training loss is 0.04419759==\n",
      "===Epoch 9000, the training loss is 0.04133422==\n",
      "===Epoch 10000, the training loss is 0.04114687==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04492461==\n",
      "===Epoch 2000, the training loss is 0.04175968==\n",
      "===Epoch 3000, the training loss is 0.04059033==\n",
      "===Epoch 4000, the training loss is 0.04027139==\n",
      "===Epoch 5000, the training loss is 0.03977090==\n",
      "===Epoch 6000, the training loss is 0.03952978==\n",
      "===Epoch 7000, the training loss is 0.03951742==\n",
      "===Epoch 8000, the training loss is 0.03931579==\n",
      "===Epoch 9000, the training loss is 0.03927043==\n",
      "===Epoch 10000, the training loss is 0.03917384==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04757188==\n",
      "===Epoch 2000, the training loss is 0.04535535==\n",
      "===Epoch 3000, the training loss is 0.04047360==\n",
      "===Epoch 4000, the training loss is 0.03703614==\n",
      "===Epoch 5000, the training loss is 0.03641881==\n",
      "===Epoch 6000, the training loss is 0.03578590==\n",
      "===Epoch 7000, the training loss is 0.03369330==\n",
      "===Epoch 8000, the training loss is 0.03262988==\n",
      "===Epoch 9000, the training loss is 0.03236488==\n",
      "===Epoch 10000, the training loss is 0.03215992==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04631492==\n",
      "===Epoch 2000, the training loss is 0.04511352==\n",
      "===Epoch 3000, the training loss is 0.04399005==\n",
      "===Epoch 4000, the training loss is 0.04332569==\n",
      "===Epoch 5000, the training loss is 0.04301995==\n",
      "===Epoch 6000, the training loss is 0.04289811==\n",
      "===Epoch 7000, the training loss is 0.04267824==\n",
      "===Epoch 8000, the training loss is 0.04251095==\n",
      "===Epoch 9000, the training loss is 0.04223760==\n",
      "===Epoch 10000, the training loss is 0.04179885==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04402733==\n",
      "===Epoch 2000, the training loss is 0.03975618==\n",
      "===Epoch 3000, the training loss is 0.03901362==\n",
      "===Epoch 4000, the training loss is 0.03476960==\n",
      "===Epoch 5000, the training loss is 0.03419210==\n",
      "===Epoch 6000, the training loss is 0.03413046==\n",
      "===Epoch 7000, the training loss is 0.03395754==\n",
      "===Epoch 8000, the training loss is 0.03377962==\n",
      "===Epoch 9000, the training loss is 0.03305135==\n",
      "===Epoch 10000, the training loss is 0.03296703==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04505039==\n",
      "===Epoch 2000, the training loss is 0.03923069==\n",
      "===Epoch 3000, the training loss is 0.03855005==\n",
      "===Epoch 4000, the training loss is 0.03679235==\n",
      "===Epoch 5000, the training loss is 0.03634822==\n",
      "===Epoch 6000, the training loss is 0.03625381==\n",
      "===Epoch 7000, the training loss is 0.03590742==\n",
      "===Epoch 8000, the training loss is 0.03583714==\n",
      "===Epoch 9000, the training loss is 0.03550768==\n",
      "===Epoch 10000, the training loss is 0.03455952==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04313535==\n",
      "===Epoch 2000, the training loss is 0.03950813==\n",
      "===Epoch 3000, the training loss is 0.03504602==\n",
      "===Epoch 4000, the training loss is 0.03415655==\n",
      "===Epoch 5000, the training loss is 0.03368860==\n",
      "===Epoch 6000, the training loss is 0.03321411==\n",
      "===Epoch 7000, the training loss is 0.03302829==\n",
      "===Epoch 8000, the training loss is 0.02934913==\n",
      "===Epoch 9000, the training loss is 0.02670682==\n",
      "===Epoch 10000, the training loss is 0.02628806==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03777555==\n",
      "===Epoch 2000, the training loss is 0.03196733==\n",
      "===Epoch 3000, the training loss is 0.03033929==\n",
      "===Epoch 4000, the training loss is 0.02695810==\n",
      "===Epoch 5000, the training loss is 0.02589635==\n",
      "===Epoch 6000, the training loss is 0.02465704==\n",
      "===Epoch 7000, the training loss is 0.02364232==\n",
      "===Epoch 8000, the training loss is 0.02307358==\n",
      "===Epoch 9000, the training loss is 0.02242677==\n",
      "===Epoch 10000, the training loss is 0.02207272==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04609036==\n",
      "===Epoch 2000, the training loss is 0.04204408==\n",
      "===Epoch 3000, the training loss is 0.03829375==\n",
      "===Epoch 4000, the training loss is 0.03723414==\n",
      "===Epoch 5000, the training loss is 0.03311472==\n",
      "===Epoch 6000, the training loss is 0.03195168==\n",
      "===Epoch 7000, the training loss is 0.03163261==\n",
      "===Epoch 8000, the training loss is 0.03108142==\n",
      "===Epoch 9000, the training loss is 0.03063403==\n",
      "===Epoch 10000, the training loss is 0.03022298==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04921517==\n",
      "===Epoch 2000, the training loss is 0.03789305==\n",
      "===Epoch 3000, the training loss is 0.03483733==\n",
      "===Epoch 4000, the training loss is 0.03426367==\n",
      "===Epoch 5000, the training loss is 0.03385811==\n",
      "===Epoch 6000, the training loss is 0.03353466==\n",
      "===Epoch 7000, the training loss is 0.02893179==\n",
      "===Epoch 8000, the training loss is 0.02798235==\n",
      "===Epoch 9000, the training loss is 0.02759932==\n",
      "===Epoch 10000, the training loss is 0.02689644==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04429452==\n",
      "===Epoch 2000, the training loss is 0.03895315==\n",
      "===Epoch 3000, the training loss is 0.03646318==\n",
      "===Epoch 4000, the training loss is 0.03551951==\n",
      "===Epoch 5000, the training loss is 0.03488334==\n",
      "===Epoch 6000, the training loss is 0.03462568==\n",
      "===Epoch 7000, the training loss is 0.03412288==\n",
      "===Epoch 8000, the training loss is 0.03213749==\n",
      "===Epoch 9000, the training loss is 0.02959862==\n",
      "===Epoch 10000, the training loss is 0.02904781==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04480420==\n",
      "===Epoch 2000, the training loss is 0.04259677==\n",
      "===Epoch 3000, the training loss is 0.04145995==\n",
      "===Epoch 4000, the training loss is 0.04114209==\n",
      "===Epoch 5000, the training loss is 0.04082275==\n",
      "===Epoch 6000, the training loss is 0.04050156==\n",
      "===Epoch 7000, the training loss is 0.04023605==\n",
      "===Epoch 8000, the training loss is 0.03701938==\n",
      "===Epoch 9000, the training loss is 0.03603537==\n",
      "===Epoch 10000, the training loss is 0.03585250==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04927805==\n",
      "===Epoch 2000, the training loss is 0.03918897==\n",
      "===Epoch 3000, the training loss is 0.03865828==\n",
      "===Epoch 4000, the training loss is 0.03798890==\n",
      "===Epoch 5000, the training loss is 0.03727542==\n",
      "===Epoch 6000, the training loss is 0.03653702==\n",
      "===Epoch 7000, the training loss is 0.03578150==\n",
      "===Epoch 8000, the training loss is 0.03528344==\n",
      "===Epoch 9000, the training loss is 0.03498109==\n",
      "===Epoch 10000, the training loss is 0.03484114==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04247303==\n",
      "===Epoch 2000, the training loss is 0.03304743==\n",
      "===Epoch 3000, the training loss is 0.02945592==\n",
      "===Epoch 4000, the training loss is 0.02855544==\n",
      "===Epoch 5000, the training loss is 0.02803386==\n",
      "===Epoch 6000, the training loss is 0.02756510==\n",
      "===Epoch 7000, the training loss is 0.02724473==\n",
      "===Epoch 8000, the training loss is 0.02530411==\n",
      "===Epoch 9000, the training loss is 0.02435403==\n",
      "===Epoch 10000, the training loss is 0.02393006==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04362047==\n",
      "===Epoch 2000, the training loss is 0.03144274==\n",
      "===Epoch 3000, the training loss is 0.02747088==\n",
      "===Epoch 4000, the training loss is 0.02654085==\n",
      "===Epoch 5000, the training loss is 0.02581947==\n",
      "===Epoch 6000, the training loss is 0.02527109==\n",
      "===Epoch 7000, the training loss is 0.02351171==\n",
      "===Epoch 8000, the training loss is 0.02323044==\n",
      "===Epoch 9000, the training loss is 0.02294945==\n",
      "===Epoch 10000, the training loss is 0.02285603==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04680310==\n",
      "===Epoch 2000, the training loss is 0.04505093==\n",
      "===Epoch 3000, the training loss is 0.04451666==\n",
      "===Epoch 4000, the training loss is 0.04406124==\n",
      "===Epoch 5000, the training loss is 0.04385432==\n",
      "===Epoch 6000, the training loss is 0.04340820==\n",
      "===Epoch 7000, the training loss is 0.04227514==\n",
      "===Epoch 8000, the training loss is 0.04207714==\n",
      "===Epoch 9000, the training loss is 0.04196643==\n",
      "===Epoch 10000, the training loss is 0.04194677==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04355309==\n",
      "===Epoch 2000, the training loss is 0.03402833==\n",
      "===Epoch 3000, the training loss is 0.03033286==\n",
      "===Epoch 4000, the training loss is 0.02882635==\n",
      "===Epoch 5000, the training loss is 0.02843956==\n",
      "===Epoch 6000, the training loss is 0.02801454==\n",
      "===Epoch 7000, the training loss is 0.02543220==\n",
      "===Epoch 8000, the training loss is 0.02513837==\n",
      "===Epoch 9000, the training loss is 0.02491926==\n",
      "===Epoch 10000, the training loss is 0.02470220==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [3:55:56<2:45:42, 4971.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04845962==\n",
      "===Epoch 2000, the training loss is 0.04201247==\n",
      "===Epoch 3000, the training loss is 0.04116666==\n",
      "===Epoch 4000, the training loss is 0.04086579==\n",
      "===Epoch 5000, the training loss is 0.04026880==\n",
      "===Epoch 6000, the training loss is 0.04013427==\n",
      "===Epoch 7000, the training loss is 0.03999443==\n",
      "===Epoch 8000, the training loss is 0.03982221==\n",
      "===Epoch 9000, the training loss is 0.03661590==\n",
      "===Epoch 10000, the training loss is 0.03615849==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04303461==\n",
      "===Epoch 2000, the training loss is 0.03702644==\n",
      "===Epoch 3000, the training loss is 0.03530314==\n",
      "===Epoch 4000, the training loss is 0.03481302==\n",
      "===Epoch 5000, the training loss is 0.03406411==\n",
      "===Epoch 6000, the training loss is 0.03363774==\n",
      "===Epoch 7000, the training loss is 0.03340196==\n",
      "===Epoch 8000, the training loss is 0.03321212==\n",
      "===Epoch 9000, the training loss is 0.03248777==\n",
      "===Epoch 10000, the training loss is 0.03229131==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04615682==\n",
      "===Epoch 2000, the training loss is 0.04482286==\n",
      "===Epoch 3000, the training loss is 0.04437838==\n",
      "===Epoch 4000, the training loss is 0.04341272==\n",
      "===Epoch 5000, the training loss is 0.04076158==\n",
      "===Epoch 6000, the training loss is 0.03981826==\n",
      "===Epoch 7000, the training loss is 0.03951219==\n",
      "===Epoch 8000, the training loss is 0.03874064==\n",
      "===Epoch 9000, the training loss is 0.03576591==\n",
      "===Epoch 10000, the training loss is 0.03533128==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03705681==\n",
      "===Epoch 2000, the training loss is 0.03480875==\n",
      "===Epoch 3000, the training loss is 0.03348131==\n",
      "===Epoch 4000, the training loss is 0.03274678==\n",
      "===Epoch 5000, the training loss is 0.03194743==\n",
      "===Epoch 6000, the training loss is 0.03162412==\n",
      "===Epoch 7000, the training loss is 0.03102583==\n",
      "===Epoch 8000, the training loss is 0.03100578==\n",
      "===Epoch 9000, the training loss is 0.03045135==\n",
      "===Epoch 10000, the training loss is 0.03030450==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04840428==\n",
      "===Epoch 2000, the training loss is 0.04456942==\n",
      "===Epoch 3000, the training loss is 0.04264189==\n",
      "===Epoch 4000, the training loss is 0.04210284==\n",
      "===Epoch 5000, the training loss is 0.04131930==\n",
      "===Epoch 6000, the training loss is 0.04070032==\n",
      "===Epoch 7000, the training loss is 0.04045897==\n",
      "===Epoch 8000, the training loss is 0.04033652==\n",
      "===Epoch 9000, the training loss is 0.04006128==\n",
      "===Epoch 10000, the training loss is 0.03912028==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04476090==\n",
      "===Epoch 2000, the training loss is 0.04251305==\n",
      "===Epoch 3000, the training loss is 0.04139911==\n",
      "===Epoch 4000, the training loss is 0.03746751==\n",
      "===Epoch 5000, the training loss is 0.03671224==\n",
      "===Epoch 6000, the training loss is 0.03578950==\n",
      "===Epoch 7000, the training loss is 0.03542576==\n",
      "===Epoch 8000, the training loss is 0.03510745==\n",
      "===Epoch 9000, the training loss is 0.03502493==\n",
      "===Epoch 10000, the training loss is 0.03490908==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04163341==\n",
      "===Epoch 2000, the training loss is 0.03712089==\n",
      "===Epoch 3000, the training loss is 0.03601453==\n",
      "===Epoch 4000, the training loss is 0.03542254==\n",
      "===Epoch 5000, the training loss is 0.03445603==\n",
      "===Epoch 6000, the training loss is 0.03357760==\n",
      "===Epoch 7000, the training loss is 0.03309004==\n",
      "===Epoch 8000, the training loss is 0.03289890==\n",
      "===Epoch 9000, the training loss is 0.03275172==\n",
      "===Epoch 10000, the training loss is 0.03262830==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04918626==\n",
      "===Epoch 2000, the training loss is 0.04499988==\n",
      "===Epoch 3000, the training loss is 0.03906582==\n",
      "===Epoch 4000, the training loss is 0.03776299==\n",
      "===Epoch 5000, the training loss is 0.03671850==\n",
      "===Epoch 6000, the training loss is 0.03637786==\n",
      "===Epoch 7000, the training loss is 0.03581555==\n",
      "===Epoch 8000, the training loss is 0.03550819==\n",
      "===Epoch 9000, the training loss is 0.03521440==\n",
      "===Epoch 10000, the training loss is 0.03504254==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04950695==\n",
      "===Epoch 2000, the training loss is 0.04789262==\n",
      "===Epoch 3000, the training loss is 0.04711801==\n",
      "===Epoch 4000, the training loss is 0.04663876==\n",
      "===Epoch 5000, the training loss is 0.04293305==\n",
      "===Epoch 6000, the training loss is 0.04167501==\n",
      "===Epoch 7000, the training loss is 0.03819312==\n",
      "===Epoch 8000, the training loss is 0.03736363==\n",
      "===Epoch 9000, the training loss is 0.03733569==\n",
      "===Epoch 10000, the training loss is 0.03720757==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04698181==\n",
      "===Epoch 2000, the training loss is 0.03549615==\n",
      "===Epoch 3000, the training loss is 0.03339923==\n",
      "===Epoch 4000, the training loss is 0.03247678==\n",
      "===Epoch 5000, the training loss is 0.03175058==\n",
      "===Epoch 6000, the training loss is 0.03126706==\n",
      "===Epoch 7000, the training loss is 0.03079024==\n",
      "===Epoch 8000, the training loss is 0.03039385==\n",
      "===Epoch 9000, the training loss is 0.03002586==\n",
      "===Epoch 10000, the training loss is 0.02999393==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04658996==\n",
      "===Epoch 2000, the training loss is 0.04110604==\n",
      "===Epoch 3000, the training loss is 0.03334670==\n",
      "===Epoch 4000, the training loss is 0.03223122==\n",
      "===Epoch 5000, the training loss is 0.03187380==\n",
      "===Epoch 6000, the training loss is 0.03159968==\n",
      "===Epoch 7000, the training loss is 0.03136100==\n",
      "===Epoch 8000, the training loss is 0.03121851==\n",
      "===Epoch 9000, the training loss is 0.03097989==\n",
      "===Epoch 10000, the training loss is 0.03000184==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04697086==\n",
      "===Epoch 2000, the training loss is 0.04070330==\n",
      "===Epoch 3000, the training loss is 0.03944981==\n",
      "===Epoch 4000, the training loss is 0.03873545==\n",
      "===Epoch 5000, the training loss is 0.03847279==\n",
      "===Epoch 6000, the training loss is 0.03808486==\n",
      "===Epoch 7000, the training loss is 0.03794868==\n",
      "===Epoch 8000, the training loss is 0.03786818==\n",
      "===Epoch 9000, the training loss is 0.03780659==\n",
      "===Epoch 10000, the training loss is 0.03772943==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04309528==\n",
      "===Epoch 2000, the training loss is 0.04170189==\n",
      "===Epoch 3000, the training loss is 0.03913536==\n",
      "===Epoch 4000, the training loss is 0.03686273==\n",
      "===Epoch 5000, the training loss is 0.03616644==\n",
      "===Epoch 6000, the training loss is 0.03584868==\n",
      "===Epoch 7000, the training loss is 0.03579806==\n",
      "===Epoch 8000, the training loss is 0.03509642==\n",
      "===Epoch 9000, the training loss is 0.03489747==\n",
      "===Epoch 10000, the training loss is 0.03468071==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04732722==\n",
      "===Epoch 2000, the training loss is 0.04468166==\n",
      "===Epoch 3000, the training loss is 0.04403878==\n",
      "===Epoch 4000, the training loss is 0.04180019==\n",
      "===Epoch 5000, the training loss is 0.04029088==\n",
      "===Epoch 6000, the training loss is 0.03878196==\n",
      "===Epoch 7000, the training loss is 0.03826047==\n",
      "===Epoch 8000, the training loss is 0.03792375==\n",
      "===Epoch 9000, the training loss is 0.03762620==\n",
      "===Epoch 10000, the training loss is 0.03759028==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04547416==\n",
      "===Epoch 2000, the training loss is 0.04254920==\n",
      "===Epoch 3000, the training loss is 0.03818966==\n",
      "===Epoch 4000, the training loss is 0.03772700==\n",
      "===Epoch 5000, the training loss is 0.03631356==\n",
      "===Epoch 6000, the training loss is 0.03580530==\n",
      "===Epoch 7000, the training loss is 0.03570243==\n",
      "===Epoch 8000, the training loss is 0.03107704==\n",
      "===Epoch 9000, the training loss is 0.02739415==\n",
      "===Epoch 10000, the training loss is 0.02696337==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04912910==\n",
      "===Epoch 2000, the training loss is 0.04735944==\n",
      "===Epoch 3000, the training loss is 0.04325938==\n",
      "===Epoch 4000, the training loss is 0.04263885==\n",
      "===Epoch 5000, the training loss is 0.03825600==\n",
      "===Epoch 6000, the training loss is 0.03684095==\n",
      "===Epoch 7000, the training loss is 0.03653204==\n",
      "===Epoch 8000, the training loss is 0.03634822==\n",
      "===Epoch 9000, the training loss is 0.03622031==\n",
      "===Epoch 10000, the training loss is 0.03608690==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04789180==\n",
      "===Epoch 2000, the training loss is 0.04616468==\n",
      "===Epoch 3000, the training loss is 0.04145879==\n",
      "===Epoch 4000, the training loss is 0.04054992==\n",
      "===Epoch 5000, the training loss is 0.04010447==\n",
      "===Epoch 6000, the training loss is 0.03983701==\n",
      "===Epoch 7000, the training loss is 0.03970674==\n",
      "===Epoch 8000, the training loss is 0.03959425==\n",
      "===Epoch 9000, the training loss is 0.03964506==\n",
      "===Epoch 10000, the training loss is 0.03934625==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04442492==\n",
      "===Epoch 2000, the training loss is 0.03939962==\n",
      "===Epoch 3000, the training loss is 0.03683969==\n",
      "===Epoch 4000, the training loss is 0.03276961==\n",
      "===Epoch 5000, the training loss is 0.03203614==\n",
      "===Epoch 6000, the training loss is 0.03164453==\n",
      "===Epoch 7000, the training loss is 0.03134995==\n",
      "===Epoch 8000, the training loss is 0.03105480==\n",
      "===Epoch 9000, the training loss is 0.03010960==\n",
      "===Epoch 10000, the training loss is 0.02985191==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04607475==\n",
      "===Epoch 2000, the training loss is 0.04456577==\n",
      "===Epoch 3000, the training loss is 0.04324420==\n",
      "===Epoch 4000, the training loss is 0.03812478==\n",
      "===Epoch 5000, the training loss is 0.03634629==\n",
      "===Epoch 6000, the training loss is 0.03566298==\n",
      "===Epoch 7000, the training loss is 0.03494310==\n",
      "===Epoch 8000, the training loss is 0.03465302==\n",
      "===Epoch 9000, the training loss is 0.03388781==\n",
      "===Epoch 10000, the training loss is 0.03325151==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04712332==\n",
      "===Epoch 2000, the training loss is 0.04128814==\n",
      "===Epoch 3000, the training loss is 0.03630508==\n",
      "===Epoch 4000, the training loss is 0.03549730==\n",
      "===Epoch 5000, the training loss is 0.03446880==\n",
      "===Epoch 6000, the training loss is 0.03404249==\n",
      "===Epoch 7000, the training loss is 0.03380308==\n",
      "===Epoch 8000, the training loss is 0.03301954==\n",
      "===Epoch 9000, the training loss is 0.03275102==\n",
      "===Epoch 10000, the training loss is 0.03222856==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [5:27:07<1:26:08, 5168.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04580513==\n",
      "===Epoch 2000, the training loss is 0.04141642==\n",
      "===Epoch 3000, the training loss is 0.04096317==\n",
      "===Epoch 4000, the training loss is 0.04083384==\n",
      "===Epoch 5000, the training loss is 0.04033573==\n",
      "===Epoch 6000, the training loss is 0.03749820==\n",
      "===Epoch 7000, the training loss is 0.03714552==\n",
      "===Epoch 8000, the training loss is 0.03703432==\n",
      "===Epoch 9000, the training loss is 0.03690158==\n",
      "===Epoch 10000, the training loss is 0.03666117==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04541587==\n",
      "===Epoch 2000, the training loss is 0.03676325==\n",
      "===Epoch 3000, the training loss is 0.03575071==\n",
      "===Epoch 4000, the training loss is 0.03478392==\n",
      "===Epoch 5000, the training loss is 0.03377628==\n",
      "===Epoch 6000, the training loss is 0.03360868==\n",
      "===Epoch 7000, the training loss is 0.02991874==\n",
      "===Epoch 8000, the training loss is 0.02972932==\n",
      "===Epoch 9000, the training loss is 0.02975477==\n",
      "===Epoch 10000, the training loss is 0.02939205==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.05044055==\n",
      "===Epoch 2000, the training loss is 0.04568261==\n",
      "===Epoch 3000, the training loss is 0.04459875==\n",
      "===Epoch 4000, the training loss is 0.04046301==\n",
      "===Epoch 5000, the training loss is 0.03857761==\n",
      "===Epoch 6000, the training loss is 0.03813673==\n",
      "===Epoch 7000, the training loss is 0.03760190==\n",
      "===Epoch 8000, the training loss is 0.03709221==\n",
      "===Epoch 9000, the training loss is 0.03699598==\n",
      "===Epoch 10000, the training loss is 0.03680763==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.05002350==\n",
      "===Epoch 2000, the training loss is 0.04845209==\n",
      "===Epoch 3000, the training loss is 0.04768446==\n",
      "===Epoch 4000, the training loss is 0.04641768==\n",
      "===Epoch 5000, the training loss is 0.04622173==\n",
      "===Epoch 6000, the training loss is 0.04247495==\n",
      "===Epoch 7000, the training loss is 0.04163554==\n",
      "===Epoch 8000, the training loss is 0.04107840==\n",
      "===Epoch 9000, the training loss is 0.04094096==\n",
      "===Epoch 10000, the training loss is 0.04006940==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04092611==\n",
      "===Epoch 2000, the training loss is 0.03565019==\n",
      "===Epoch 3000, the training loss is 0.03373950==\n",
      "===Epoch 4000, the training loss is 0.02902940==\n",
      "===Epoch 5000, the training loss is 0.02860707==\n",
      "===Epoch 6000, the training loss is 0.02812625==\n",
      "===Epoch 7000, the training loss is 0.02764877==\n",
      "===Epoch 8000, the training loss is 0.02731356==\n",
      "===Epoch 9000, the training loss is 0.02706689==\n",
      "===Epoch 10000, the training loss is 0.02684414==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04769269==\n",
      "===Epoch 2000, the training loss is 0.03911915==\n",
      "===Epoch 3000, the training loss is 0.03394786==\n",
      "===Epoch 4000, the training loss is 0.03335969==\n",
      "===Epoch 5000, the training loss is 0.03303153==\n",
      "===Epoch 6000, the training loss is 0.03278121==\n",
      "===Epoch 7000, the training loss is 0.03257486==\n",
      "===Epoch 8000, the training loss is 0.03198926==\n",
      "===Epoch 9000, the training loss is 0.03131619==\n",
      "===Epoch 10000, the training loss is 0.03103990==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.05052402==\n",
      "===Epoch 2000, the training loss is 0.04896135==\n",
      "===Epoch 3000, the training loss is 0.04573784==\n",
      "===Epoch 4000, the training loss is 0.04512948==\n",
      "===Epoch 5000, the training loss is 0.04353820==\n",
      "===Epoch 6000, the training loss is 0.04313902==\n",
      "===Epoch 7000, the training loss is 0.04241934==\n",
      "===Epoch 8000, the training loss is 0.04228178==\n",
      "===Epoch 9000, the training loss is 0.04159917==\n",
      "===Epoch 10000, the training loss is 0.04142276==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04726947==\n",
      "===Epoch 2000, the training loss is 0.04460691==\n",
      "===Epoch 3000, the training loss is 0.04382546==\n",
      "===Epoch 4000, the training loss is 0.04359121==\n",
      "===Epoch 5000, the training loss is 0.04337669==\n",
      "===Epoch 6000, the training loss is 0.04273884==\n",
      "===Epoch 7000, the training loss is 0.04250571==\n",
      "===Epoch 8000, the training loss is 0.04217753==\n",
      "===Epoch 9000, the training loss is 0.04172882==\n",
      "===Epoch 10000, the training loss is 0.04153309==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.05002541==\n",
      "===Epoch 2000, the training loss is 0.04898284==\n",
      "===Epoch 3000, the training loss is 0.04643756==\n",
      "===Epoch 4000, the training loss is 0.04505991==\n",
      "===Epoch 5000, the training loss is 0.04207199==\n",
      "===Epoch 6000, the training loss is 0.03969519==\n",
      "===Epoch 7000, the training loss is 0.03905943==\n",
      "===Epoch 8000, the training loss is 0.03885396==\n",
      "===Epoch 9000, the training loss is 0.03845670==\n",
      "===Epoch 10000, the training loss is 0.03777229==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04796476==\n",
      "===Epoch 2000, the training loss is 0.04636573==\n",
      "===Epoch 3000, the training loss is 0.04551841==\n",
      "===Epoch 4000, the training loss is 0.04077353==\n",
      "===Epoch 5000, the training loss is 0.03959677==\n",
      "===Epoch 6000, the training loss is 0.03862097==\n",
      "===Epoch 7000, the training loss is 0.03805676==\n",
      "===Epoch 8000, the training loss is 0.03749856==\n",
      "===Epoch 9000, the training loss is 0.03726040==\n",
      "===Epoch 10000, the training loss is 0.03682680==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04570906==\n",
      "===Epoch 2000, the training loss is 0.03944369==\n",
      "===Epoch 3000, the training loss is 0.03861646==\n",
      "===Epoch 4000, the training loss is 0.03807792==\n",
      "===Epoch 5000, the training loss is 0.03695089==\n",
      "===Epoch 6000, the training loss is 0.03658890==\n",
      "===Epoch 7000, the training loss is 0.03635984==\n",
      "===Epoch 8000, the training loss is 0.03359750==\n",
      "===Epoch 9000, the training loss is 0.03326678==\n",
      "===Epoch 10000, the training loss is 0.03307931==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04350210==\n",
      "===Epoch 2000, the training loss is 0.04027468==\n",
      "===Epoch 3000, the training loss is 0.03540820==\n",
      "===Epoch 4000, the training loss is 0.03459455==\n",
      "===Epoch 5000, the training loss is 0.03397542==\n",
      "===Epoch 6000, the training loss is 0.03199862==\n",
      "===Epoch 7000, the training loss is 0.03179513==\n",
      "===Epoch 8000, the training loss is 0.03164190==\n",
      "===Epoch 9000, the training loss is 0.03116447==\n",
      "===Epoch 10000, the training loss is 0.03091708==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04993566==\n",
      "===Epoch 2000, the training loss is 0.04811786==\n",
      "===Epoch 3000, the training loss is 0.04720654==\n",
      "===Epoch 4000, the training loss is 0.04680402==\n",
      "===Epoch 5000, the training loss is 0.04661880==\n",
      "===Epoch 6000, the training loss is 0.04383694==\n",
      "===Epoch 7000, the training loss is 0.04337663==\n",
      "===Epoch 8000, the training loss is 0.04152336==\n",
      "===Epoch 9000, the training loss is 0.04142706==\n",
      "===Epoch 10000, the training loss is 0.04131544==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.05037104==\n",
      "===Epoch 2000, the training loss is 0.04934318==\n",
      "===Epoch 3000, the training loss is 0.04349539==\n",
      "===Epoch 4000, the training loss is 0.04318342==\n",
      "===Epoch 5000, the training loss is 0.04301390==\n",
      "===Epoch 6000, the training loss is 0.04285187==\n",
      "===Epoch 7000, the training loss is 0.04271130==\n",
      "===Epoch 8000, the training loss is 0.04239155==\n",
      "===Epoch 9000, the training loss is 0.04223463==\n",
      "===Epoch 10000, the training loss is 0.04207848==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04590163==\n",
      "===Epoch 2000, the training loss is 0.04397248==\n",
      "===Epoch 3000, the training loss is 0.04251443==\n",
      "===Epoch 4000, the training loss is 0.04217984==\n",
      "===Epoch 5000, the training loss is 0.04190332==\n",
      "===Epoch 6000, the training loss is 0.04121475==\n",
      "===Epoch 7000, the training loss is 0.04091838==\n",
      "===Epoch 8000, the training loss is 0.03651489==\n",
      "===Epoch 9000, the training loss is 0.03618404==\n",
      "===Epoch 10000, the training loss is 0.03591741==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03970820==\n",
      "===Epoch 2000, the training loss is 0.03318877==\n",
      "===Epoch 3000, the training loss is 0.03208853==\n",
      "===Epoch 4000, the training loss is 0.03134178==\n",
      "===Epoch 5000, the training loss is 0.03034598==\n",
      "===Epoch 6000, the training loss is 0.02940500==\n",
      "===Epoch 7000, the training loss is 0.02893721==\n",
      "===Epoch 8000, the training loss is 0.02884205==\n",
      "===Epoch 9000, the training loss is 0.02861376==\n",
      "===Epoch 10000, the training loss is 0.02842543==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04872883==\n",
      "===Epoch 2000, the training loss is 0.04766733==\n",
      "===Epoch 3000, the training loss is 0.04622810==\n",
      "===Epoch 4000, the training loss is 0.04459194==\n",
      "===Epoch 5000, the training loss is 0.04439702==\n",
      "===Epoch 6000, the training loss is 0.04416553==\n",
      "===Epoch 7000, the training loss is 0.04372275==\n",
      "===Epoch 8000, the training loss is 0.04360253==\n",
      "===Epoch 9000, the training loss is 0.04281875==\n",
      "===Epoch 10000, the training loss is 0.04262745==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04658220==\n",
      "===Epoch 2000, the training loss is 0.04409809==\n",
      "===Epoch 3000, the training loss is 0.04315814==\n",
      "===Epoch 4000, the training loss is 0.04189184==\n",
      "===Epoch 5000, the training loss is 0.04152967==\n",
      "===Epoch 6000, the training loss is 0.04125697==\n",
      "===Epoch 7000, the training loss is 0.04117255==\n",
      "===Epoch 8000, the training loss is 0.04108803==\n",
      "===Epoch 9000, the training loss is 0.04100228==\n",
      "===Epoch 10000, the training loss is 0.04095117==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04696078==\n",
      "===Epoch 2000, the training loss is 0.04590294==\n",
      "===Epoch 3000, the training loss is 0.04520754==\n",
      "===Epoch 4000, the training loss is 0.04473889==\n",
      "===Epoch 5000, the training loss is 0.04355497==\n",
      "===Epoch 6000, the training loss is 0.04294309==\n",
      "===Epoch 7000, the training loss is 0.04282001==\n",
      "===Epoch 8000, the training loss is 0.04260691==\n",
      "===Epoch 9000, the training loss is 0.04253449==\n",
      "===Epoch 10000, the training loss is 0.04249986==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04683398==\n",
      "===Epoch 2000, the training loss is 0.04282893==\n",
      "===Epoch 3000, the training loss is 0.03794990==\n",
      "===Epoch 4000, the training loss is 0.03704994==\n",
      "===Epoch 5000, the training loss is 0.03600087==\n",
      "===Epoch 6000, the training loss is 0.03561555==\n",
      "===Epoch 7000, the training loss is 0.03524962==\n",
      "===Epoch 8000, the training loss is 0.03455650==\n",
      "===Epoch 9000, the training loss is 0.03341135==\n",
      "===Epoch 10000, the training loss is 0.03200512==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [6:58:36<00:00, 5023.29s/it]  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### input\n",
    "size = 512\n",
    "l = 1\n",
    "d = defaultdict(list)\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    for a in [0.6, 0.7, 0.8, 0.9]:\n",
    "        \n",
    "        metaload_path = '/mnt/cloud1/sheng-projects/st_projects/spatial_clust/spatial-clust-scripts/ipynb/Bokai_reorg/benchmark/spleen/data/'\n",
    "        df_clean = pd.read_csv('/mnt/cloud1/sheng-projects/st_projects/spatial_clust/spatial-clust-scripts/ipynb/Bokai_reorg/spleen/data/features_and_metadata.csv', index_col=0)\n",
    "        features = np.load('/mnt/cloud1/sheng-projects/st_projects/spatial_clust/spatial-clust-scripts/ipynb/Bokai_reorg/spleen/data/feature_scaled.npy')\n",
    "        \n",
    "        cell_nbhd = np.load(os.path.join(metaload_path,  f\"cell_nbhd_res0.5_k20_forimg.npy\")) # all the same for imgs\n",
    "        train_mask = np.load(os.path.join(metaload_path,  \"train_mask.npy\"))\n",
    "        feature_labels = np.load(os.path.join(metaload_path,  \"feature_labels_res0.5_forimg.npy\"))\n",
    "        feature_edges = np.load(os.path.join(metaload_path,  \"feature_edges_res0.5_forimg.npy\"))\n",
    "        spatial_edges = np.load(os.path.join(metaload_path,  \"spatial_edges_0326.npy\"))                       \n",
    "                               \n",
    "        # change into torch\n",
    "        features = torch.from_numpy(features).float().to(args.device)\n",
    "        feat_edge_index = torch.from_numpy(np.array(feature_edges.T[:2])).long().to(args.device)\n",
    "        spat_edge_index = torch.from_numpy(np.array(spatial_edges.T[:2])).long().to(args.device)\n",
    "        \n",
    "        # combo nbhd                       \n",
    "        df_clean['res'] = feature_labels\n",
    "        reslabel = pd.get_dummies(df_clean['res'])\n",
    "        combo_nbhd = np.hstack([reslabel, cell_nbhd])\n",
    "        combo_nbhd = torch.from_numpy(combo_nbhd).float().to(args.device)\n",
    "        \n",
    "        ## cnn\n",
    "        load_path = '/mnt/cloud1/sheng-projects/st_projects/spatial_clust/spatial-clust-scripts/ipynb/Bokai_reorg/benchmark/spleen/data/'\n",
    "        save_folder = os.path.join(load_path, \"cnn\", f\"cnn_512_l{l}_layer6_testalpha:{a}_checkpoints\", \"epochs\", 'embed')\n",
    "        args.out_dim = combo_nbhd.shape[1]\n",
    "        \n",
    "        #### reset args here\n",
    "        class Args:\n",
    "            gnn_input_dim = 31\n",
    "            cnn_input_dim = 128\n",
    "            fc_dim = latent_dim = 32\n",
    "            cnn_dim = cnn_latent_dim = 32\n",
    "            out_dim = combo_nbhd.shape[1]\n",
    "\n",
    "            fc_out_dim = 33\n",
    "            cnn_out_dim = 11\n",
    "            hid_out_dim = 33\n",
    "\n",
    "            criterion = \"L1\"\n",
    "            learning_rate = 1e-3\n",
    "            epochs = 10000\n",
    "            print_every = 1000\n",
    "            average_iter = 100\n",
    "            device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        args = Args()\n",
    "        #### reseat args finished\n",
    "        \n",
    "        # get cnn embedding\n",
    "        epoch = 400\n",
    "        cnn_embedding = np.load(os.path.join(save_folder, f'cnn_512_testalpha:{a}_l1_layer6_byepoch' ,f\"cnn_embedding_512_full_l1_dim128_epoch{epoch}.npy\"))\n",
    "        cnn_embedding = torch.from_numpy(cnn_embedding).float().to(args.device)\n",
    "        cnn = cnn_embedding\n",
    "        \n",
    "        stable = True\n",
    "        if stable:\n",
    "            rep = 5\n",
    "            dim = args.fc_out_dim + args.cnn_out_dim\n",
    "            concat_embedding = np.zeros((features.shape[0], rep * dim))\n",
    "            for i in range(rep):\n",
    "                print(i)\n",
    "                gnn_embedding = get_gnn_embed(SNAP_GNN(args), combo_nbhd, features, cnn, feat_edge_index, spat_edge_index, verbose=True)\n",
    "                concat_embedding[:, i*dim : (i+1)*dim] = gnn_embedding\n",
    "            Ue, Se, Vhe = np.linalg.svd(concat_embedding, full_matrices=False)\n",
    "\n",
    "            plt.plot(Se)\n",
    "            k = 32\n",
    "            gnn_embedding = Ue[:, :k] @ np.diag(Se[:k])\n",
    "        else:\n",
    "            gnn_embedding = get_gnn_embed(SNAP_GNN(args), combo_nbhd, features, cnn, feat_edge_index, spat_edge_index, verbose=True)\n",
    "\n",
    "        ## save out\n",
    "        dir = '../data/saved_embedding/img_alpha' + str(a) + '_dbGNN_0327.npy'\n",
    "        np.save(dir, gnn_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellsnap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
