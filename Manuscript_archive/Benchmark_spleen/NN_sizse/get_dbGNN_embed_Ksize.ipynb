{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bokai/miniconda3/envs/cellsnap/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import sys\n",
    "sys.path.append(\"../../../spatial-clust-scripts-main/\")\n",
    "import model\n",
    "import warnings\n",
    "import numpy as np\n",
    "import leidenalg\n",
    "import igraph as ig\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from collections import defaultdict\n",
    "import graph\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import skimage\n",
    "# import custom functions\n",
    "import utils\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import umap\n",
    "# from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from validclust import dunn\n",
    "from sklearn.metrics import pairwise_distances\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## during testing only test snap_gnn\n",
    "\n",
    "class SNAP_GNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_features=args.gnn_input_dim, out_features=args.fc_dim)\n",
    "        self.cnn_fc = nn.Linear(in_features=args.cnn_input_dim, out_features=args.cnn_dim)\n",
    "        self.feat_conv1 = GCNConv(args.fc_dim, args.latent_dim)\n",
    "        self.feat_conv2 = GCNConv(args.latent_dim, args.fc_out_dim)\n",
    "        \n",
    "        self.spat_conv1 = GCNConv(args.cnn_dim, args.cnn_latent_dim)\n",
    "        self.spat_conv2 = GCNConv(args.cnn_latent_dim, args.cnn_out_dim)\n",
    "        \n",
    "        self.proj1 = nn.Linear(in_features=args.fc_out_dim+args.cnn_out_dim, \n",
    "                              out_features=args.hid_out_dim)\n",
    "        self.proj2 = nn.Linear(in_features=args.hid_out_dim, \n",
    "                              out_features=args.out_dim)\n",
    "\n",
    "    def feat_gnn_encoder(self, feat, feat_edge_index):\n",
    "        feat = F.relu(self.fc(feat))\n",
    "        feat = F.relu(self.feat_conv1(feat, feat_edge_index))\n",
    "        feat = self.feat_conv2(feat, feat_edge_index)\n",
    "        \n",
    "        return feat\n",
    "    \n",
    "    def spat_gnn_encoder(self, spat, spat_edge_index):\n",
    "        spat = F.relu(self.cnn_fc(spat))\n",
    "        spat = F.relu(self.spat_conv1(spat, spat_edge_index))\n",
    "        spat = self.spat_conv2(spat, spat_edge_index)\n",
    "        \n",
    "        return spat\n",
    "    \n",
    "    def encoder(self, feat, spat, feat_edge_index, spat_edge_index):\n",
    "        x_feat = self.feat_gnn_encoder(feat, feat_edge_index)\n",
    "        x_spat = self.spat_gnn_encoder(spat, spat_edge_index)\n",
    "        x = torch.cat((x_feat, x_spat), dim = 1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def forward(self, feat, spat, feat_edge_index, spat_edge_index):\n",
    "        x = F.relu(self.encoder(feat, spat, feat_edge_index, spat_edge_index))\n",
    "        x = self.proj1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.proj2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    gnn_input_dim = 31\n",
    "    cnn_input_dim = 128\n",
    "    fc_dim = latent_dim = 32\n",
    "    cnn_dim = cnn_latent_dim = 32\n",
    "   # out_dim = 14 * 2 # dont know this value yet\n",
    "    fc_out_dim = 33\n",
    "    cnn_out_dim = 11\n",
    "    hid_out_dim = 33\n",
    "\n",
    "    criterion = \"L1\"\n",
    "    learning_rate = 1e-3\n",
    "    epochs = 10000\n",
    "    print_every = 1000\n",
    "    average_iter = 100\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gnn_embed(model, cell_nbhd, feat, spat, feat_edge_index, spat_edge_index, verbose=False):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    cell_nbhd = cell_nbhd.to(args.device)\n",
    "    model = model.to(args.device)\n",
    "    if args.criterion == \"L1\":\n",
    "        print(\"Use L1 Loss\")\n",
    "        criterion = nn.L1Loss()\n",
    "    elif args.criterion == \"L2\":\n",
    "        print(\"Use L2 Loss\")\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        print(\"Cross Entropy\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loss_epoch = []\n",
    "    #criterion = nn.L1Loss()\n",
    "    for e in range(1, 1+args.epochs):\n",
    "        model.train()\n",
    "        predicted_nbhd = model(features, cnn_embedding, feat_edge_index, spat_edge_index)\n",
    "        # Compute prediction error\n",
    "        loss = criterion(predicted_nbhd, cell_nbhd)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # take one step\n",
    "        optimizer.step()\n",
    "\n",
    "        # record the loss\n",
    "        curr_train_loss = loss.item()\n",
    "        if verbose and e % args.print_every  == 0:\n",
    "            print(f'===Epoch {e}, the training loss is {curr_train_loss:>0.8f}==', flush=True)\n",
    "        train_loss_epoch.append(curr_train_loss)\n",
    "    return model.encoder(feat, spat, feat_edge_index, spat_edge_index).cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/bokai/miniconda3/envs/cellsnap/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04721655==\n",
      "===Epoch 2000, the training loss is 0.04508208==\n",
      "===Epoch 3000, the training loss is 0.04301231==\n",
      "===Epoch 4000, the training loss is 0.04255247==\n",
      "===Epoch 5000, the training loss is 0.04234456==\n",
      "===Epoch 6000, the training loss is 0.04161169==\n",
      "===Epoch 7000, the training loss is 0.04136091==\n",
      "===Epoch 8000, the training loss is 0.04092125==\n",
      "===Epoch 9000, the training loss is 0.04024490==\n",
      "===Epoch 10000, the training loss is 0.04015519==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04834428==\n",
      "===Epoch 2000, the training loss is 0.04683596==\n",
      "===Epoch 3000, the training loss is 0.04635492==\n",
      "===Epoch 4000, the training loss is 0.04301682==\n",
      "===Epoch 5000, the training loss is 0.04136979==\n",
      "===Epoch 6000, the training loss is 0.04080314==\n",
      "===Epoch 7000, the training loss is 0.04050158==\n",
      "===Epoch 8000, the training loss is 0.04031947==\n",
      "===Epoch 9000, the training loss is 0.04017578==\n",
      "===Epoch 10000, the training loss is 0.04006065==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04699483==\n",
      "===Epoch 2000, the training loss is 0.04628016==\n",
      "===Epoch 3000, the training loss is 0.04519472==\n",
      "===Epoch 4000, the training loss is 0.04496543==\n",
      "===Epoch 5000, the training loss is 0.04444047==\n",
      "===Epoch 6000, the training loss is 0.04215284==\n",
      "===Epoch 7000, the training loss is 0.04199082==\n",
      "===Epoch 8000, the training loss is 0.04156559==\n",
      "===Epoch 9000, the training loss is 0.04109879==\n",
      "===Epoch 10000, the training loss is 0.03942331==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04504550==\n",
      "===Epoch 2000, the training loss is 0.03912092==\n",
      "===Epoch 3000, the training loss is 0.03699754==\n",
      "===Epoch 4000, the training loss is 0.03466748==\n",
      "===Epoch 5000, the training loss is 0.03383425==\n",
      "===Epoch 6000, the training loss is 0.03353460==\n",
      "===Epoch 7000, the training loss is 0.03212356==\n",
      "===Epoch 8000, the training loss is 0.03123808==\n",
      "===Epoch 9000, the training loss is 0.02884748==\n",
      "===Epoch 10000, the training loss is 0.02851272==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.05331627==\n",
      "===Epoch 2000, the training loss is 0.05011567==\n",
      "===Epoch 3000, the training loss is 0.04766535==\n",
      "===Epoch 4000, the training loss is 0.04331893==\n",
      "===Epoch 5000, the training loss is 0.04268247==\n",
      "===Epoch 6000, the training loss is 0.04237638==\n",
      "===Epoch 7000, the training loss is 0.04234517==\n",
      "===Epoch 8000, the training loss is 0.04216181==\n",
      "===Epoch 9000, the training loss is 0.04206168==\n",
      "===Epoch 10000, the training loss is 0.04186067==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04133995==\n",
      "===Epoch 2000, the training loss is 0.03215667==\n",
      "===Epoch 3000, the training loss is 0.03064714==\n",
      "===Epoch 4000, the training loss is 0.03014880==\n",
      "===Epoch 5000, the training loss is 0.02985823==\n",
      "===Epoch 6000, the training loss is 0.02957778==\n",
      "===Epoch 7000, the training loss is 0.02896155==\n",
      "===Epoch 8000, the training loss is 0.02656217==\n",
      "===Epoch 9000, the training loss is 0.02558524==\n",
      "===Epoch 10000, the training loss is 0.02529665==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03431527==\n",
      "===Epoch 2000, the training loss is 0.03168945==\n",
      "===Epoch 3000, the training loss is 0.02813235==\n",
      "===Epoch 4000, the training loss is 0.02708276==\n",
      "===Epoch 5000, the training loss is 0.02648094==\n",
      "===Epoch 6000, the training loss is 0.02572838==\n",
      "===Epoch 7000, the training loss is 0.02543044==\n",
      "===Epoch 8000, the training loss is 0.02516322==\n",
      "===Epoch 9000, the training loss is 0.02496664==\n",
      "===Epoch 10000, the training loss is 0.02469997==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.05057020==\n",
      "===Epoch 2000, the training loss is 0.04832201==\n",
      "===Epoch 3000, the training loss is 0.04151006==\n",
      "===Epoch 4000, the training loss is 0.03874638==\n",
      "===Epoch 5000, the training loss is 0.03848599==\n",
      "===Epoch 6000, the training loss is 0.03838765==\n",
      "===Epoch 7000, the training loss is 0.03448391==\n",
      "===Epoch 8000, the training loss is 0.03390827==\n",
      "===Epoch 9000, the training loss is 0.03380359==\n",
      "===Epoch 10000, the training loss is 0.03356223==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04529952==\n",
      "===Epoch 2000, the training loss is 0.04356389==\n",
      "===Epoch 3000, the training loss is 0.04282948==\n",
      "===Epoch 4000, the training loss is 0.04277560==\n",
      "===Epoch 5000, the training loss is 0.04194871==\n",
      "===Epoch 6000, the training loss is 0.04160642==\n",
      "===Epoch 7000, the training loss is 0.04133231==\n",
      "===Epoch 8000, the training loss is 0.04075337==\n",
      "===Epoch 9000, the training loss is 0.04041243==\n",
      "===Epoch 10000, the training loss is 0.04028043==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04056282==\n",
      "===Epoch 2000, the training loss is 0.03719596==\n",
      "===Epoch 3000, the training loss is 0.03533717==\n",
      "===Epoch 4000, the training loss is 0.03504948==\n",
      "===Epoch 5000, the training loss is 0.03493530==\n",
      "===Epoch 6000, the training loss is 0.03484724==\n",
      "===Epoch 7000, the training loss is 0.03465054==\n",
      "===Epoch 8000, the training loss is 0.03452948==\n",
      "===Epoch 9000, the training loss is 0.03449351==\n",
      "===Epoch 10000, the training loss is 0.03458027==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04182225==\n",
      "===Epoch 2000, the training loss is 0.03773937==\n",
      "===Epoch 3000, the training loss is 0.03400817==\n",
      "===Epoch 4000, the training loss is 0.03335360==\n",
      "===Epoch 5000, the training loss is 0.03298403==\n",
      "===Epoch 6000, the training loss is 0.03282416==\n",
      "===Epoch 7000, the training loss is 0.03195610==\n",
      "===Epoch 8000, the training loss is 0.03071507==\n",
      "===Epoch 9000, the training loss is 0.02994568==\n",
      "===Epoch 10000, the training loss is 0.02959186==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03704031==\n",
      "===Epoch 2000, the training loss is 0.03452594==\n",
      "===Epoch 3000, the training loss is 0.03251927==\n",
      "===Epoch 4000, the training loss is 0.03165619==\n",
      "===Epoch 5000, the training loss is 0.03099813==\n",
      "===Epoch 6000, the training loss is 0.03052751==\n",
      "===Epoch 7000, the training loss is 0.03013246==\n",
      "===Epoch 8000, the training loss is 0.03004258==\n",
      "===Epoch 9000, the training loss is 0.02989909==\n",
      "===Epoch 10000, the training loss is 0.02985408==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03640349==\n",
      "===Epoch 2000, the training loss is 0.03461741==\n",
      "===Epoch 3000, the training loss is 0.03426572==\n",
      "===Epoch 4000, the training loss is 0.03383960==\n",
      "===Epoch 5000, the training loss is 0.03277652==\n",
      "===Epoch 6000, the training loss is 0.03119155==\n",
      "===Epoch 7000, the training loss is 0.03075559==\n",
      "===Epoch 8000, the training loss is 0.03056444==\n",
      "===Epoch 9000, the training loss is 0.03040799==\n",
      "===Epoch 10000, the training loss is 0.03033126==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03998890==\n",
      "===Epoch 2000, the training loss is 0.03505704==\n",
      "===Epoch 3000, the training loss is 0.03305487==\n",
      "===Epoch 4000, the training loss is 0.03240689==\n",
      "===Epoch 5000, the training loss is 0.03155566==\n",
      "===Epoch 6000, the training loss is 0.03112352==\n",
      "===Epoch 7000, the training loss is 0.03094011==\n",
      "===Epoch 8000, the training loss is 0.03083830==\n",
      "===Epoch 9000, the training loss is 0.03046073==\n",
      "===Epoch 10000, the training loss is 0.03015279==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04846565==\n",
      "===Epoch 2000, the training loss is 0.04179211==\n",
      "===Epoch 3000, the training loss is 0.03934003==\n",
      "===Epoch 4000, the training loss is 0.03659715==\n",
      "===Epoch 5000, the training loss is 0.03597678==\n",
      "===Epoch 6000, the training loss is 0.03557396==\n",
      "===Epoch 7000, the training loss is 0.03440279==\n",
      "===Epoch 8000, the training loss is 0.03427187==\n",
      "===Epoch 9000, the training loss is 0.03397560==\n",
      "===Epoch 10000, the training loss is 0.03226777==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03750865==\n",
      "===Epoch 2000, the training loss is 0.03318861==\n",
      "===Epoch 3000, the training loss is 0.02937060==\n",
      "===Epoch 4000, the training loss is 0.02835193==\n",
      "===Epoch 5000, the training loss is 0.02829774==\n",
      "===Epoch 6000, the training loss is 0.02815436==\n",
      "===Epoch 7000, the training loss is 0.02799054==\n",
      "===Epoch 8000, the training loss is 0.02794872==\n",
      "===Epoch 9000, the training loss is 0.02785063==\n",
      "===Epoch 10000, the training loss is 0.02783059==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03839307==\n",
      "===Epoch 2000, the training loss is 0.02728514==\n",
      "===Epoch 3000, the training loss is 0.02585149==\n",
      "===Epoch 4000, the training loss is 0.02464460==\n",
      "===Epoch 5000, the training loss is 0.02405166==\n",
      "===Epoch 6000, the training loss is 0.02155539==\n",
      "===Epoch 7000, the training loss is 0.02023099==\n",
      "===Epoch 8000, the training loss is 0.01982937==\n",
      "===Epoch 9000, the training loss is 0.01972049==\n",
      "===Epoch 10000, the training loss is 0.01953675==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03572467==\n",
      "===Epoch 2000, the training loss is 0.03421772==\n",
      "===Epoch 3000, the training loss is 0.02951851==\n",
      "===Epoch 4000, the training loss is 0.02822571==\n",
      "===Epoch 5000, the training loss is 0.02699398==\n",
      "===Epoch 6000, the training loss is 0.02664220==\n",
      "===Epoch 7000, the training loss is 0.02654563==\n",
      "===Epoch 8000, the training loss is 0.02580129==\n",
      "===Epoch 9000, the training loss is 0.02487151==\n",
      "===Epoch 10000, the training loss is 0.02375114==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03735581==\n",
      "===Epoch 2000, the training loss is 0.03616758==\n",
      "===Epoch 3000, the training loss is 0.03450846==\n",
      "===Epoch 4000, the training loss is 0.03210618==\n",
      "===Epoch 5000, the training loss is 0.02836613==\n",
      "===Epoch 6000, the training loss is 0.02759803==\n",
      "===Epoch 7000, the training loss is 0.02463550==\n",
      "===Epoch 8000, the training loss is 0.02203186==\n",
      "===Epoch 9000, the training loss is 0.02141326==\n",
      "===Epoch 10000, the training loss is 0.02115895==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03479993==\n",
      "===Epoch 2000, the training loss is 0.02405290==\n",
      "===Epoch 3000, the training loss is 0.02069134==\n",
      "===Epoch 4000, the training loss is 0.01914872==\n",
      "===Epoch 5000, the training loss is 0.01868316==\n",
      "===Epoch 6000, the training loss is 0.01826985==\n",
      "===Epoch 7000, the training loss is 0.01779377==\n",
      "===Epoch 8000, the training loss is 0.01749945==\n",
      "===Epoch 9000, the training loss is 0.01720974==\n",
      "===Epoch 10000, the training loss is 0.01698043==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [1:31:22<6:05:31, 5482.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04534286==\n",
      "===Epoch 2000, the training loss is 0.04287701==\n",
      "===Epoch 3000, the training loss is 0.04116910==\n",
      "===Epoch 4000, the training loss is 0.04066407==\n",
      "===Epoch 5000, the training loss is 0.03815186==\n",
      "===Epoch 6000, the training loss is 0.03770843==\n",
      "===Epoch 7000, the training loss is 0.03754169==\n",
      "===Epoch 8000, the training loss is 0.03737909==\n",
      "===Epoch 9000, the training loss is 0.03736639==\n",
      "===Epoch 10000, the training loss is 0.03730641==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04775659==\n",
      "===Epoch 2000, the training loss is 0.04469216==\n",
      "===Epoch 3000, the training loss is 0.04413529==\n",
      "===Epoch 4000, the training loss is 0.04322212==\n",
      "===Epoch 5000, the training loss is 0.04294666==\n",
      "===Epoch 6000, the training loss is 0.04292244==\n",
      "===Epoch 7000, the training loss is 0.04280830==\n",
      "===Epoch 8000, the training loss is 0.04269550==\n",
      "===Epoch 9000, the training loss is 0.04270703==\n",
      "===Epoch 10000, the training loss is 0.04252414==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04701730==\n",
      "===Epoch 2000, the training loss is 0.04601273==\n",
      "===Epoch 3000, the training loss is 0.04305092==\n",
      "===Epoch 4000, the training loss is 0.04110928==\n",
      "===Epoch 5000, the training loss is 0.04057859==\n",
      "===Epoch 6000, the training loss is 0.04021645==\n",
      "===Epoch 7000, the training loss is 0.03961643==\n",
      "===Epoch 8000, the training loss is 0.03958921==\n",
      "===Epoch 9000, the training loss is 0.03952589==\n",
      "===Epoch 10000, the training loss is 0.03947534==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.05212853==\n",
      "===Epoch 2000, the training loss is 0.04737744==\n",
      "===Epoch 3000, the training loss is 0.04566453==\n",
      "===Epoch 4000, the training loss is 0.04519786==\n",
      "===Epoch 5000, the training loss is 0.04415623==\n",
      "===Epoch 6000, the training loss is 0.04401673==\n",
      "===Epoch 7000, the training loss is 0.04382083==\n",
      "===Epoch 8000, the training loss is 0.04375484==\n",
      "===Epoch 9000, the training loss is 0.04367106==\n",
      "===Epoch 10000, the training loss is 0.04363462==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04472324==\n",
      "===Epoch 2000, the training loss is 0.03833911==\n",
      "===Epoch 3000, the training loss is 0.03682122==\n",
      "===Epoch 4000, the training loss is 0.03640821==\n",
      "===Epoch 5000, the training loss is 0.03599734==\n",
      "===Epoch 6000, the training loss is 0.03023630==\n",
      "===Epoch 7000, the training loss is 0.02986767==\n",
      "===Epoch 8000, the training loss is 0.02936152==\n",
      "===Epoch 9000, the training loss is 0.02894886==\n",
      "===Epoch 10000, the training loss is 0.02886944==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03953478==\n",
      "===Epoch 2000, the training loss is 0.03657024==\n",
      "===Epoch 3000, the training loss is 0.03553802==\n",
      "===Epoch 4000, the training loss is 0.03462288==\n",
      "===Epoch 5000, the training loss is 0.03419208==\n",
      "===Epoch 6000, the training loss is 0.03397987==\n",
      "===Epoch 7000, the training loss is 0.03278116==\n",
      "===Epoch 8000, the training loss is 0.03077409==\n",
      "===Epoch 9000, the training loss is 0.03035326==\n",
      "===Epoch 10000, the training loss is 0.03021690==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04088765==\n",
      "===Epoch 2000, the training loss is 0.03818407==\n",
      "===Epoch 3000, the training loss is 0.03766283==\n",
      "===Epoch 4000, the training loss is 0.03724173==\n",
      "===Epoch 5000, the training loss is 0.03696300==\n",
      "===Epoch 6000, the training loss is 0.03698320==\n",
      "===Epoch 7000, the training loss is 0.03670650==\n",
      "===Epoch 8000, the training loss is 0.03660041==\n",
      "===Epoch 9000, the training loss is 0.03366399==\n",
      "===Epoch 10000, the training loss is 0.03324343==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04018582==\n",
      "===Epoch 2000, the training loss is 0.03939795==\n",
      "===Epoch 3000, the training loss is 0.03783137==\n",
      "===Epoch 4000, the training loss is 0.03700181==\n",
      "===Epoch 5000, the training loss is 0.03629281==\n",
      "===Epoch 6000, the training loss is 0.03590900==\n",
      "===Epoch 7000, the training loss is 0.03545551==\n",
      "===Epoch 8000, the training loss is 0.03492292==\n",
      "===Epoch 9000, the training loss is 0.03469848==\n",
      "===Epoch 10000, the training loss is 0.03462463==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04107501==\n",
      "===Epoch 2000, the training loss is 0.03868963==\n",
      "===Epoch 3000, the training loss is 0.03828820==\n",
      "===Epoch 4000, the training loss is 0.03790585==\n",
      "===Epoch 5000, the training loss is 0.03769999==\n",
      "===Epoch 6000, the training loss is 0.03562791==\n",
      "===Epoch 7000, the training loss is 0.03461049==\n",
      "===Epoch 8000, the training loss is 0.03442758==\n",
      "===Epoch 9000, the training loss is 0.03419723==\n",
      "===Epoch 10000, the training loss is 0.03392236==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04095182==\n",
      "===Epoch 2000, the training loss is 0.03924990==\n",
      "===Epoch 3000, the training loss is 0.03619909==\n",
      "===Epoch 4000, the training loss is 0.03482310==\n",
      "===Epoch 5000, the training loss is 0.03468589==\n",
      "===Epoch 6000, the training loss is 0.03445914==\n",
      "===Epoch 7000, the training loss is 0.03397150==\n",
      "===Epoch 8000, the training loss is 0.02868628==\n",
      "===Epoch 9000, the training loss is 0.02777041==\n",
      "===Epoch 10000, the training loss is 0.02754414==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03919879==\n",
      "===Epoch 2000, the training loss is 0.03346132==\n",
      "===Epoch 3000, the training loss is 0.03266403==\n",
      "===Epoch 4000, the training loss is 0.03023233==\n",
      "===Epoch 5000, the training loss is 0.02923256==\n",
      "===Epoch 6000, the training loss is 0.02896093==\n",
      "===Epoch 7000, the training loss is 0.02870696==\n",
      "===Epoch 8000, the training loss is 0.02860304==\n",
      "===Epoch 9000, the training loss is 0.02856901==\n",
      "===Epoch 10000, the training loss is 0.02840704==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03804291==\n",
      "===Epoch 2000, the training loss is 0.03536390==\n",
      "===Epoch 3000, the training loss is 0.03445489==\n",
      "===Epoch 4000, the training loss is 0.03368002==\n",
      "===Epoch 5000, the training loss is 0.03310456==\n",
      "===Epoch 6000, the training loss is 0.03289248==\n",
      "===Epoch 7000, the training loss is 0.03277979==\n",
      "===Epoch 8000, the training loss is 0.03259896==\n",
      "===Epoch 9000, the training loss is 0.03262739==\n",
      "===Epoch 10000, the training loss is 0.03241511==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04246481==\n",
      "===Epoch 2000, the training loss is 0.03811561==\n",
      "===Epoch 3000, the training loss is 0.03609324==\n",
      "===Epoch 4000, the training loss is 0.02954777==\n",
      "===Epoch 5000, the training loss is 0.02769961==\n",
      "===Epoch 6000, the training loss is 0.02627904==\n",
      "===Epoch 7000, the training loss is 0.02555526==\n",
      "===Epoch 8000, the training loss is 0.02478631==\n",
      "===Epoch 9000, the training loss is 0.02409532==\n",
      "===Epoch 10000, the training loss is 0.02401043==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04091627==\n",
      "===Epoch 2000, the training loss is 0.03785420==\n",
      "===Epoch 3000, the training loss is 0.03719261==\n",
      "===Epoch 4000, the training loss is 0.03425860==\n",
      "===Epoch 5000, the training loss is 0.03396181==\n",
      "===Epoch 6000, the training loss is 0.03382058==\n",
      "===Epoch 7000, the training loss is 0.03361931==\n",
      "===Epoch 8000, the training loss is 0.03330238==\n",
      "===Epoch 9000, the training loss is 0.03259594==\n",
      "===Epoch 10000, the training loss is 0.02988431==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03888763==\n",
      "===Epoch 2000, the training loss is 0.03186071==\n",
      "===Epoch 3000, the training loss is 0.02584858==\n",
      "===Epoch 4000, the training loss is 0.02441806==\n",
      "===Epoch 5000, the training loss is 0.02339273==\n",
      "===Epoch 6000, the training loss is 0.02308765==\n",
      "===Epoch 7000, the training loss is 0.02265866==\n",
      "===Epoch 8000, the training loss is 0.02248298==\n",
      "===Epoch 9000, the training loss is 0.02240880==\n",
      "===Epoch 10000, the training loss is 0.02208269==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04021496==\n",
      "===Epoch 2000, the training loss is 0.03854363==\n",
      "===Epoch 3000, the training loss is 0.03686169==\n",
      "===Epoch 4000, the training loss is 0.03402469==\n",
      "===Epoch 5000, the training loss is 0.03302769==\n",
      "===Epoch 6000, the training loss is 0.03211118==\n",
      "===Epoch 7000, the training loss is 0.03167027==\n",
      "===Epoch 8000, the training loss is 0.02864977==\n",
      "===Epoch 9000, the training loss is 0.02840209==\n",
      "===Epoch 10000, the training loss is 0.02825633==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03672819==\n",
      "===Epoch 2000, the training loss is 0.03133240==\n",
      "===Epoch 3000, the training loss is 0.02892863==\n",
      "===Epoch 4000, the training loss is 0.02621575==\n",
      "===Epoch 5000, the training loss is 0.02571914==\n",
      "===Epoch 6000, the training loss is 0.02522565==\n",
      "===Epoch 7000, the training loss is 0.02449245==\n",
      "===Epoch 8000, the training loss is 0.02391376==\n",
      "===Epoch 9000, the training loss is 0.01998316==\n",
      "===Epoch 10000, the training loss is 0.01931990==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03422997==\n",
      "===Epoch 2000, the training loss is 0.03054410==\n",
      "===Epoch 3000, the training loss is 0.02918812==\n",
      "===Epoch 4000, the training loss is 0.02881801==\n",
      "===Epoch 5000, the training loss is 0.02459777==\n",
      "===Epoch 6000, the training loss is 0.02345823==\n",
      "===Epoch 7000, the training loss is 0.02317759==\n",
      "===Epoch 8000, the training loss is 0.02097599==\n",
      "===Epoch 9000, the training loss is 0.02040898==\n",
      "===Epoch 10000, the training loss is 0.02007998==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03303954==\n",
      "===Epoch 2000, the training loss is 0.03138302==\n",
      "===Epoch 3000, the training loss is 0.03068865==\n",
      "===Epoch 4000, the training loss is 0.02982421==\n",
      "===Epoch 5000, the training loss is 0.02947107==\n",
      "===Epoch 6000, the training loss is 0.02699761==\n",
      "===Epoch 7000, the training loss is 0.02663893==\n",
      "===Epoch 8000, the training loss is 0.02636848==\n",
      "===Epoch 9000, the training loss is 0.02607482==\n",
      "===Epoch 10000, the training loss is 0.02593607==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03165476==\n",
      "===Epoch 2000, the training loss is 0.02862393==\n",
      "===Epoch 3000, the training loss is 0.02698494==\n",
      "===Epoch 4000, the training loss is 0.02637084==\n",
      "===Epoch 5000, the training loss is 0.02623851==\n",
      "===Epoch 6000, the training loss is 0.02611591==\n",
      "===Epoch 7000, the training loss is 0.02475841==\n",
      "===Epoch 8000, the training loss is 0.02183018==\n",
      "===Epoch 9000, the training loss is 0.02122258==\n",
      "===Epoch 10000, the training loss is 0.02089607==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [3:02:46<4:34:09, 5483.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04745880==\n",
      "===Epoch 2000, the training loss is 0.04587256==\n",
      "===Epoch 3000, the training loss is 0.04332621==\n",
      "===Epoch 4000, the training loss is 0.03986221==\n",
      "===Epoch 5000, the training loss is 0.03956188==\n",
      "===Epoch 6000, the training loss is 0.03921856==\n",
      "===Epoch 7000, the training loss is 0.03887167==\n",
      "===Epoch 8000, the training loss is 0.03872542==\n",
      "===Epoch 9000, the training loss is 0.03861338==\n",
      "===Epoch 10000, the training loss is 0.03847064==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04713959==\n",
      "===Epoch 2000, the training loss is 0.04291797==\n",
      "===Epoch 3000, the training loss is 0.04219447==\n",
      "===Epoch 4000, the training loss is 0.04194954==\n",
      "===Epoch 5000, the training loss is 0.04189160==\n",
      "===Epoch 6000, the training loss is 0.04044348==\n",
      "===Epoch 7000, the training loss is 0.03972926==\n",
      "===Epoch 8000, the training loss is 0.03962438==\n",
      "===Epoch 9000, the training loss is 0.03945399==\n",
      "===Epoch 10000, the training loss is 0.03895358==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04405390==\n",
      "===Epoch 2000, the training loss is 0.04251468==\n",
      "===Epoch 3000, the training loss is 0.04189853==\n",
      "===Epoch 4000, the training loss is 0.04139856==\n",
      "===Epoch 5000, the training loss is 0.03970235==\n",
      "===Epoch 6000, the training loss is 0.03926679==\n",
      "===Epoch 7000, the training loss is 0.03841120==\n",
      "===Epoch 8000, the training loss is 0.03807602==\n",
      "===Epoch 9000, the training loss is 0.03777764==\n",
      "===Epoch 10000, the training loss is 0.03774682==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04090548==\n",
      "===Epoch 2000, the training loss is 0.03703374==\n",
      "===Epoch 3000, the training loss is 0.03563962==\n",
      "===Epoch 4000, the training loss is 0.03254139==\n",
      "===Epoch 5000, the training loss is 0.03197636==\n",
      "===Epoch 6000, the training loss is 0.03153920==\n",
      "===Epoch 7000, the training loss is 0.03139336==\n",
      "===Epoch 8000, the training loss is 0.03059123==\n",
      "===Epoch 9000, the training loss is 0.03031311==\n",
      "===Epoch 10000, the training loss is 0.02999077==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04992487==\n",
      "===Epoch 2000, the training loss is 0.04654987==\n",
      "===Epoch 3000, the training loss is 0.04397787==\n",
      "===Epoch 4000, the training loss is 0.04255642==\n",
      "===Epoch 5000, the training loss is 0.04141069==\n",
      "===Epoch 6000, the training loss is 0.04111774==\n",
      "===Epoch 7000, the training loss is 0.04084767==\n",
      "===Epoch 8000, the training loss is 0.04072109==\n",
      "===Epoch 9000, the training loss is 0.04060784==\n",
      "===Epoch 10000, the training loss is 0.04053294==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04163016==\n",
      "===Epoch 2000, the training loss is 0.03815183==\n",
      "===Epoch 3000, the training loss is 0.03624876==\n",
      "===Epoch 4000, the training loss is 0.03009984==\n",
      "===Epoch 5000, the training loss is 0.02871207==\n",
      "===Epoch 6000, the training loss is 0.02804094==\n",
      "===Epoch 7000, the training loss is 0.02777136==\n",
      "===Epoch 8000, the training loss is 0.02743721==\n",
      "===Epoch 9000, the training loss is 0.02704245==\n",
      "===Epoch 10000, the training loss is 0.02553259==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04146864==\n",
      "===Epoch 2000, the training loss is 0.04063277==\n",
      "===Epoch 3000, the training loss is 0.03835363==\n",
      "===Epoch 4000, the training loss is 0.03776880==\n",
      "===Epoch 5000, the training loss is 0.03758521==\n",
      "===Epoch 6000, the training loss is 0.03729860==\n",
      "===Epoch 7000, the training loss is 0.03718285==\n",
      "===Epoch 8000, the training loss is 0.03657715==\n",
      "===Epoch 9000, the training loss is 0.03649375==\n",
      "===Epoch 10000, the training loss is 0.03629948==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04499389==\n",
      "===Epoch 2000, the training loss is 0.04203755==\n",
      "===Epoch 3000, the training loss is 0.03789603==\n",
      "===Epoch 4000, the training loss is 0.03433781==\n",
      "===Epoch 5000, the training loss is 0.03375404==\n",
      "===Epoch 6000, the training loss is 0.03358845==\n",
      "===Epoch 7000, the training loss is 0.03274849==\n",
      "===Epoch 8000, the training loss is 0.03256045==\n",
      "===Epoch 9000, the training loss is 0.03246765==\n",
      "===Epoch 10000, the training loss is 0.03227677==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04508435==\n",
      "===Epoch 2000, the training loss is 0.04214450==\n",
      "===Epoch 3000, the training loss is 0.03867685==\n",
      "===Epoch 4000, the training loss is 0.03848249==\n",
      "===Epoch 5000, the training loss is 0.03745057==\n",
      "===Epoch 6000, the training loss is 0.03614480==\n",
      "===Epoch 7000, the training loss is 0.03436800==\n",
      "===Epoch 8000, the training loss is 0.03430676==\n",
      "===Epoch 9000, the training loss is 0.03395335==\n",
      "===Epoch 10000, the training loss is 0.03397593==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04255353==\n",
      "===Epoch 2000, the training loss is 0.04049461==\n",
      "===Epoch 3000, the training loss is 0.03956761==\n",
      "===Epoch 4000, the training loss is 0.03876131==\n",
      "===Epoch 5000, the training loss is 0.03826142==\n",
      "===Epoch 6000, the training loss is 0.03727318==\n",
      "===Epoch 7000, the training loss is 0.03692574==\n",
      "===Epoch 8000, the training loss is 0.03683758==\n",
      "===Epoch 9000, the training loss is 0.03658361==\n",
      "===Epoch 10000, the training loss is 0.03654804==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04913935==\n",
      "===Epoch 2000, the training loss is 0.04799870==\n",
      "===Epoch 3000, the training loss is 0.04643568==\n",
      "===Epoch 4000, the training loss is 0.03911580==\n",
      "===Epoch 5000, the training loss is 0.03871235==\n",
      "===Epoch 6000, the training loss is 0.03850928==\n",
      "===Epoch 7000, the training loss is 0.03800964==\n",
      "===Epoch 8000, the training loss is 0.03632530==\n",
      "===Epoch 9000, the training loss is 0.03620117==\n",
      "===Epoch 10000, the training loss is 0.03616191==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04322327==\n",
      "===Epoch 2000, the training loss is 0.04089221==\n",
      "===Epoch 3000, the training loss is 0.04032836==\n",
      "===Epoch 4000, the training loss is 0.03948770==\n",
      "===Epoch 5000, the training loss is 0.03849433==\n",
      "===Epoch 6000, the training loss is 0.03707951==\n",
      "===Epoch 7000, the training loss is 0.03622118==\n",
      "===Epoch 8000, the training loss is 0.03576019==\n",
      "===Epoch 9000, the training loss is 0.03538634==\n",
      "===Epoch 10000, the training loss is 0.03525103==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04069861==\n",
      "===Epoch 2000, the training loss is 0.03066901==\n",
      "===Epoch 3000, the training loss is 0.02832755==\n",
      "===Epoch 4000, the training loss is 0.02790720==\n",
      "===Epoch 5000, the training loss is 0.02769973==\n",
      "===Epoch 6000, the training loss is 0.02719523==\n",
      "===Epoch 7000, the training loss is 0.02709657==\n",
      "===Epoch 8000, the training loss is 0.02669606==\n",
      "===Epoch 9000, the training loss is 0.02645084==\n",
      "===Epoch 10000, the training loss is 0.02624066==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04502000==\n",
      "===Epoch 2000, the training loss is 0.04160000==\n",
      "===Epoch 3000, the training loss is 0.03461738==\n",
      "===Epoch 4000, the training loss is 0.03338182==\n",
      "===Epoch 5000, the training loss is 0.03301509==\n",
      "===Epoch 6000, the training loss is 0.03258061==\n",
      "===Epoch 7000, the training loss is 0.03010218==\n",
      "===Epoch 8000, the training loss is 0.02896978==\n",
      "===Epoch 9000, the training loss is 0.02892291==\n",
      "===Epoch 10000, the training loss is 0.02755783==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04605838==\n",
      "===Epoch 2000, the training loss is 0.04081713==\n",
      "===Epoch 3000, the training loss is 0.03983246==\n",
      "===Epoch 4000, the training loss is 0.03962833==\n",
      "===Epoch 5000, the training loss is 0.03386757==\n",
      "===Epoch 6000, the training loss is 0.03285041==\n",
      "===Epoch 7000, the training loss is 0.03252865==\n",
      "===Epoch 8000, the training loss is 0.03045761==\n",
      "===Epoch 9000, the training loss is 0.03013254==\n",
      "===Epoch 10000, the training loss is 0.02994232==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03511396==\n",
      "===Epoch 2000, the training loss is 0.03369473==\n",
      "===Epoch 3000, the training loss is 0.03136569==\n",
      "===Epoch 4000, the training loss is 0.03083997==\n",
      "===Epoch 5000, the training loss is 0.03038024==\n",
      "===Epoch 6000, the training loss is 0.02587922==\n",
      "===Epoch 7000, the training loss is 0.02465604==\n",
      "===Epoch 8000, the training loss is 0.02418659==\n",
      "===Epoch 9000, the training loss is 0.02411473==\n",
      "===Epoch 10000, the training loss is 0.02392037==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03496142==\n",
      "===Epoch 2000, the training loss is 0.02921856==\n",
      "===Epoch 3000, the training loss is 0.02453935==\n",
      "===Epoch 4000, the training loss is 0.02351738==\n",
      "===Epoch 5000, the training loss is 0.02255274==\n",
      "===Epoch 6000, the training loss is 0.02176166==\n",
      "===Epoch 7000, the training loss is 0.02073664==\n",
      "===Epoch 8000, the training loss is 0.02032829==\n",
      "===Epoch 9000, the training loss is 0.02005219==\n",
      "===Epoch 10000, the training loss is 0.01955902==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03802603==\n",
      "===Epoch 2000, the training loss is 0.03329996==\n",
      "===Epoch 3000, the training loss is 0.02982983==\n",
      "===Epoch 4000, the training loss is 0.02876652==\n",
      "===Epoch 5000, the training loss is 0.02821275==\n",
      "===Epoch 6000, the training loss is 0.02801679==\n",
      "===Epoch 7000, the training loss is 0.02785316==\n",
      "===Epoch 8000, the training loss is 0.02780731==\n",
      "===Epoch 9000, the training loss is 0.02680494==\n",
      "===Epoch 10000, the training loss is 0.02669921==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03601292==\n",
      "===Epoch 2000, the training loss is 0.03065662==\n",
      "===Epoch 3000, the training loss is 0.02921448==\n",
      "===Epoch 4000, the training loss is 0.02796240==\n",
      "===Epoch 5000, the training loss is 0.02729501==\n",
      "===Epoch 6000, the training loss is 0.02655924==\n",
      "===Epoch 7000, the training loss is 0.02628689==\n",
      "===Epoch 8000, the training loss is 0.02602827==\n",
      "===Epoch 9000, the training loss is 0.02598617==\n",
      "===Epoch 10000, the training loss is 0.02572566==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03823686==\n",
      "===Epoch 2000, the training loss is 0.02834170==\n",
      "===Epoch 3000, the training loss is 0.02548218==\n",
      "===Epoch 4000, the training loss is 0.02432075==\n",
      "===Epoch 5000, the training loss is 0.02364252==\n",
      "===Epoch 6000, the training loss is 0.02318191==\n",
      "===Epoch 7000, the training loss is 0.02211269==\n",
      "===Epoch 8000, the training loss is 0.02117500==\n",
      "===Epoch 9000, the training loss is 0.02101184==\n",
      "===Epoch 10000, the training loss is 0.02091882==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [4:34:03<3:02:40, 5480.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04982055==\n",
      "===Epoch 2000, the training loss is 0.04584401==\n",
      "===Epoch 3000, the training loss is 0.04187442==\n",
      "===Epoch 4000, the training loss is 0.04057460==\n",
      "===Epoch 5000, the training loss is 0.03783383==\n",
      "===Epoch 6000, the training loss is 0.03753454==\n",
      "===Epoch 7000, the training loss is 0.03738760==\n",
      "===Epoch 8000, the training loss is 0.03723042==\n",
      "===Epoch 9000, the training loss is 0.03622195==\n",
      "===Epoch 10000, the training loss is 0.03587558==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04749012==\n",
      "===Epoch 2000, the training loss is 0.04524036==\n",
      "===Epoch 3000, the training loss is 0.04170797==\n",
      "===Epoch 4000, the training loss is 0.04115979==\n",
      "===Epoch 5000, the training loss is 0.04060963==\n",
      "===Epoch 6000, the training loss is 0.04024152==\n",
      "===Epoch 7000, the training loss is 0.04010327==\n",
      "===Epoch 8000, the training loss is 0.03996927==\n",
      "===Epoch 9000, the training loss is 0.04001094==\n",
      "===Epoch 10000, the training loss is 0.03990622==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04685344==\n",
      "===Epoch 2000, the training loss is 0.03744279==\n",
      "===Epoch 3000, the training loss is 0.03622332==\n",
      "===Epoch 4000, the training loss is 0.03523115==\n",
      "===Epoch 5000, the training loss is 0.03298546==\n",
      "===Epoch 6000, the training loss is 0.03254536==\n",
      "===Epoch 7000, the training loss is 0.03223964==\n",
      "===Epoch 8000, the training loss is 0.03211650==\n",
      "===Epoch 9000, the training loss is 0.03194701==\n",
      "===Epoch 10000, the training loss is 0.03161617==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04430666==\n",
      "===Epoch 2000, the training loss is 0.03889369==\n",
      "===Epoch 3000, the training loss is 0.03792232==\n",
      "===Epoch 4000, the training loss is 0.03726840==\n",
      "===Epoch 5000, the training loss is 0.03681540==\n",
      "===Epoch 6000, the training loss is 0.03353004==\n",
      "===Epoch 7000, the training loss is 0.03338655==\n",
      "===Epoch 8000, the training loss is 0.03318740==\n",
      "===Epoch 9000, the training loss is 0.03302157==\n",
      "===Epoch 10000, the training loss is 0.03310414==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04710194==\n",
      "===Epoch 2000, the training loss is 0.04366415==\n",
      "===Epoch 3000, the training loss is 0.04317134==\n",
      "===Epoch 4000, the training loss is 0.04301845==\n",
      "===Epoch 5000, the training loss is 0.04268580==\n",
      "===Epoch 6000, the training loss is 0.04194071==\n",
      "===Epoch 7000, the training loss is 0.04162622==\n",
      "===Epoch 8000, the training loss is 0.04158463==\n",
      "===Epoch 9000, the training loss is 0.04145682==\n",
      "===Epoch 10000, the training loss is 0.04107929==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04551736==\n",
      "===Epoch 2000, the training loss is 0.04219923==\n",
      "===Epoch 3000, the training loss is 0.04094944==\n",
      "===Epoch 4000, the training loss is 0.04038977==\n",
      "===Epoch 5000, the training loss is 0.04016291==\n",
      "===Epoch 6000, the training loss is 0.03938706==\n",
      "===Epoch 7000, the training loss is 0.03744372==\n",
      "===Epoch 8000, the training loss is 0.03707230==\n",
      "===Epoch 9000, the training loss is 0.03630190==\n",
      "===Epoch 10000, the training loss is 0.03598617==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03693362==\n",
      "===Epoch 2000, the training loss is 0.03382684==\n",
      "===Epoch 3000, the training loss is 0.03222365==\n",
      "===Epoch 4000, the training loss is 0.03156178==\n",
      "===Epoch 5000, the training loss is 0.02508135==\n",
      "===Epoch 6000, the training loss is 0.02350158==\n",
      "===Epoch 7000, the training loss is 0.02257700==\n",
      "===Epoch 8000, the training loss is 0.02240570==\n",
      "===Epoch 9000, the training loss is 0.02221767==\n",
      "===Epoch 10000, the training loss is 0.02203016==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03919639==\n",
      "===Epoch 2000, the training loss is 0.03231747==\n",
      "===Epoch 3000, the training loss is 0.02902005==\n",
      "===Epoch 4000, the training loss is 0.02802170==\n",
      "===Epoch 5000, the training loss is 0.02737544==\n",
      "===Epoch 6000, the training loss is 0.02713687==\n",
      "===Epoch 7000, the training loss is 0.02684379==\n",
      "===Epoch 8000, the training loss is 0.02671727==\n",
      "===Epoch 9000, the training loss is 0.02667505==\n",
      "===Epoch 10000, the training loss is 0.02649612==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04079810==\n",
      "===Epoch 2000, the training loss is 0.03564639==\n",
      "===Epoch 3000, the training loss is 0.03358300==\n",
      "===Epoch 4000, the training loss is 0.03324555==\n",
      "===Epoch 5000, the training loss is 0.03269675==\n",
      "===Epoch 6000, the training loss is 0.03162217==\n",
      "===Epoch 7000, the training loss is 0.03064620==\n",
      "===Epoch 8000, the training loss is 0.03043881==\n",
      "===Epoch 9000, the training loss is 0.03011486==\n",
      "===Epoch 10000, the training loss is 0.02983004==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03614116==\n",
      "===Epoch 2000, the training loss is 0.03437606==\n",
      "===Epoch 3000, the training loss is 0.03157330==\n",
      "===Epoch 4000, the training loss is 0.03080312==\n",
      "===Epoch 5000, the training loss is 0.03002398==\n",
      "===Epoch 6000, the training loss is 0.02905475==\n",
      "===Epoch 7000, the training loss is 0.02424428==\n",
      "===Epoch 8000, the training loss is 0.02404556==\n",
      "===Epoch 9000, the training loss is 0.02380303==\n",
      "===Epoch 10000, the training loss is 0.02368368==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03935970==\n",
      "===Epoch 2000, the training loss is 0.03717764==\n",
      "===Epoch 3000, the training loss is 0.03524706==\n",
      "===Epoch 4000, the training loss is 0.03099222==\n",
      "===Epoch 5000, the training loss is 0.03048783==\n",
      "===Epoch 6000, the training loss is 0.03010277==\n",
      "===Epoch 7000, the training loss is 0.02983004==\n",
      "===Epoch 8000, the training loss is 0.02926457==\n",
      "===Epoch 9000, the training loss is 0.02877904==\n",
      "===Epoch 10000, the training loss is 0.02869078==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03644725==\n",
      "===Epoch 2000, the training loss is 0.03440382==\n",
      "===Epoch 3000, the training loss is 0.03112866==\n",
      "===Epoch 4000, the training loss is 0.02708906==\n",
      "===Epoch 5000, the training loss is 0.02653899==\n",
      "===Epoch 6000, the training loss is 0.02633779==\n",
      "===Epoch 7000, the training loss is 0.02605472==\n",
      "===Epoch 8000, the training loss is 0.02564869==\n",
      "===Epoch 9000, the training loss is 0.02558154==\n",
      "===Epoch 10000, the training loss is 0.02545284==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04305078==\n",
      "===Epoch 2000, the training loss is 0.03596755==\n",
      "===Epoch 3000, the training loss is 0.03330181==\n",
      "===Epoch 4000, the training loss is 0.03254775==\n",
      "===Epoch 5000, the training loss is 0.03258666==\n",
      "===Epoch 6000, the training loss is 0.03178969==\n",
      "===Epoch 7000, the training loss is 0.03149750==\n",
      "===Epoch 8000, the training loss is 0.03134191==\n",
      "===Epoch 9000, the training loss is 0.03121295==\n",
      "===Epoch 10000, the training loss is 0.03083730==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03916056==\n",
      "===Epoch 2000, the training loss is 0.03296908==\n",
      "===Epoch 3000, the training loss is 0.03209365==\n",
      "===Epoch 4000, the training loss is 0.03148454==\n",
      "===Epoch 5000, the training loss is 0.03066515==\n",
      "===Epoch 6000, the training loss is 0.03058655==\n",
      "===Epoch 7000, the training loss is 0.03050115==\n",
      "===Epoch 8000, the training loss is 0.02910225==\n",
      "===Epoch 9000, the training loss is 0.02878710==\n",
      "===Epoch 10000, the training loss is 0.02873831==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03493280==\n",
      "===Epoch 2000, the training loss is 0.02982323==\n",
      "===Epoch 3000, the training loss is 0.02851691==\n",
      "===Epoch 4000, the training loss is 0.02532279==\n",
      "===Epoch 5000, the training loss is 0.02387319==\n",
      "===Epoch 6000, the training loss is 0.02325177==\n",
      "===Epoch 7000, the training loss is 0.02267707==\n",
      "===Epoch 8000, the training loss is 0.02245325==\n",
      "===Epoch 9000, the training loss is 0.02182356==\n",
      "===Epoch 10000, the training loss is 0.01966111==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03407076==\n",
      "===Epoch 2000, the training loss is 0.03117257==\n",
      "===Epoch 3000, the training loss is 0.02684605==\n",
      "===Epoch 4000, the training loss is 0.02545548==\n",
      "===Epoch 5000, the training loss is 0.02513682==\n",
      "===Epoch 6000, the training loss is 0.02468496==\n",
      "===Epoch 7000, the training loss is 0.02415551==\n",
      "===Epoch 8000, the training loss is 0.02367304==\n",
      "===Epoch 9000, the training loss is 0.02198425==\n",
      "===Epoch 10000, the training loss is 0.02019743==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03584968==\n",
      "===Epoch 2000, the training loss is 0.03106860==\n",
      "===Epoch 3000, the training loss is 0.03043932==\n",
      "===Epoch 4000, the training loss is 0.02994970==\n",
      "===Epoch 5000, the training loss is 0.02975137==\n",
      "===Epoch 6000, the training loss is 0.02972157==\n",
      "===Epoch 7000, the training loss is 0.02963656==\n",
      "===Epoch 8000, the training loss is 0.02945462==\n",
      "===Epoch 9000, the training loss is 0.02937310==\n",
      "===Epoch 10000, the training loss is 0.02930250==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03738990==\n",
      "===Epoch 2000, the training loss is 0.02868284==\n",
      "===Epoch 3000, the training loss is 0.02611292==\n",
      "===Epoch 4000, the training loss is 0.02512378==\n",
      "===Epoch 5000, the training loss is 0.02469336==\n",
      "===Epoch 6000, the training loss is 0.02243402==\n",
      "===Epoch 7000, the training loss is 0.02182756==\n",
      "===Epoch 8000, the training loss is 0.02159707==\n",
      "===Epoch 9000, the training loss is 0.02105844==\n",
      "===Epoch 10000, the training loss is 0.02126619==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04497721==\n",
      "===Epoch 2000, the training loss is 0.04425383==\n",
      "===Epoch 3000, the training loss is 0.03682525==\n",
      "===Epoch 4000, the training loss is 0.03607252==\n",
      "===Epoch 5000, the training loss is 0.03535739==\n",
      "===Epoch 6000, the training loss is 0.03532779==\n",
      "===Epoch 7000, the training loss is 0.03492180==\n",
      "===Epoch 8000, the training loss is 0.03423207==\n",
      "===Epoch 9000, the training loss is 0.03387889==\n",
      "===Epoch 10000, the training loss is 0.03369152==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04324575==\n",
      "===Epoch 2000, the training loss is 0.04104476==\n",
      "===Epoch 3000, the training loss is 0.04041054==\n",
      "===Epoch 4000, the training loss is 0.04016764==\n",
      "===Epoch 5000, the training loss is 0.03541481==\n",
      "===Epoch 6000, the training loss is 0.03460948==\n",
      "===Epoch 7000, the training loss is 0.03416313==\n",
      "===Epoch 8000, the training loss is 0.03396758==\n",
      "===Epoch 9000, the training loss is 0.03390349==\n",
      "===Epoch 10000, the training loss is 0.03389020==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [6:05:30<1:31:23, 5483.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04794642==\n",
      "===Epoch 2000, the training loss is 0.04281535==\n",
      "===Epoch 3000, the training loss is 0.04137348==\n",
      "===Epoch 4000, the training loss is 0.04003298==\n",
      "===Epoch 5000, the training loss is 0.03946289==\n",
      "===Epoch 6000, the training loss is 0.03626073==\n",
      "===Epoch 7000, the training loss is 0.03596866==\n",
      "===Epoch 8000, the training loss is 0.03546042==\n",
      "===Epoch 9000, the training loss is 0.03540862==\n",
      "===Epoch 10000, the training loss is 0.03519991==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04464914==\n",
      "===Epoch 2000, the training loss is 0.04266391==\n",
      "===Epoch 3000, the training loss is 0.04150100==\n",
      "===Epoch 4000, the training loss is 0.04122227==\n",
      "===Epoch 5000, the training loss is 0.04105370==\n",
      "===Epoch 6000, the training loss is 0.04088373==\n",
      "===Epoch 7000, the training loss is 0.04087478==\n",
      "===Epoch 8000, the training loss is 0.04073184==\n",
      "===Epoch 9000, the training loss is 0.04029854==\n",
      "===Epoch 10000, the training loss is 0.04020900==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04870344==\n",
      "===Epoch 2000, the training loss is 0.04295454==\n",
      "===Epoch 3000, the training loss is 0.04159892==\n",
      "===Epoch 4000, the training loss is 0.04031686==\n",
      "===Epoch 5000, the training loss is 0.03937525==\n",
      "===Epoch 6000, the training loss is 0.03897338==\n",
      "===Epoch 7000, the training loss is 0.03783422==\n",
      "===Epoch 8000, the training loss is 0.03725193==\n",
      "===Epoch 9000, the training loss is 0.03700987==\n",
      "===Epoch 10000, the training loss is 0.03692832==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04541144==\n",
      "===Epoch 2000, the training loss is 0.04212578==\n",
      "===Epoch 3000, the training loss is 0.04047206==\n",
      "===Epoch 4000, the training loss is 0.03923418==\n",
      "===Epoch 5000, the training loss is 0.03888329==\n",
      "===Epoch 6000, the training loss is 0.03863277==\n",
      "===Epoch 7000, the training loss is 0.03864667==\n",
      "===Epoch 8000, the training loss is 0.03841589==\n",
      "===Epoch 9000, the training loss is 0.03830413==\n",
      "===Epoch 10000, the training loss is 0.03826579==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04629935==\n",
      "===Epoch 2000, the training loss is 0.04268882==\n",
      "===Epoch 3000, the training loss is 0.04036342==\n",
      "===Epoch 4000, the training loss is 0.03530649==\n",
      "===Epoch 5000, the training loss is 0.03483532==\n",
      "===Epoch 6000, the training loss is 0.03408770==\n",
      "===Epoch 7000, the training loss is 0.03390488==\n",
      "===Epoch 8000, the training loss is 0.03363129==\n",
      "===Epoch 9000, the training loss is 0.03273507==\n",
      "===Epoch 10000, the training loss is 0.03242221==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04316205==\n",
      "===Epoch 2000, the training loss is 0.03500815==\n",
      "===Epoch 3000, the training loss is 0.03277803==\n",
      "===Epoch 4000, the training loss is 0.03233537==\n",
      "===Epoch 5000, the training loss is 0.03196175==\n",
      "===Epoch 6000, the training loss is 0.03154218==\n",
      "===Epoch 7000, the training loss is 0.03134724==\n",
      "===Epoch 8000, the training loss is 0.03105651==\n",
      "===Epoch 9000, the training loss is 0.03073812==\n",
      "===Epoch 10000, the training loss is 0.03051757==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04172688==\n",
      "===Epoch 2000, the training loss is 0.03655101==\n",
      "===Epoch 3000, the training loss is 0.03482390==\n",
      "===Epoch 4000, the training loss is 0.03436513==\n",
      "===Epoch 5000, the training loss is 0.03401869==\n",
      "===Epoch 6000, the training loss is 0.03373249==\n",
      "===Epoch 7000, the training loss is 0.03291845==\n",
      "===Epoch 8000, the training loss is 0.03234330==\n",
      "===Epoch 9000, the training loss is 0.02789599==\n",
      "===Epoch 10000, the training loss is 0.02644878==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04651450==\n",
      "===Epoch 2000, the training loss is 0.03823281==\n",
      "===Epoch 3000, the training loss is 0.03695816==\n",
      "===Epoch 4000, the training loss is 0.03651174==\n",
      "===Epoch 5000, the training loss is 0.03619299==\n",
      "===Epoch 6000, the training loss is 0.03596660==\n",
      "===Epoch 7000, the training loss is 0.03571881==\n",
      "===Epoch 8000, the training loss is 0.03562126==\n",
      "===Epoch 9000, the training loss is 0.03542100==\n",
      "===Epoch 10000, the training loss is 0.03508329==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03639664==\n",
      "===Epoch 2000, the training loss is 0.03059781==\n",
      "===Epoch 3000, the training loss is 0.02750021==\n",
      "===Epoch 4000, the training loss is 0.02674523==\n",
      "===Epoch 5000, the training loss is 0.02604204==\n",
      "===Epoch 6000, the training loss is 0.02484395==\n",
      "===Epoch 7000, the training loss is 0.02443787==\n",
      "===Epoch 8000, the training loss is 0.02370986==\n",
      "===Epoch 9000, the training loss is 0.02362240==\n",
      "===Epoch 10000, the training loss is 0.02354692==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04015360==\n",
      "===Epoch 2000, the training loss is 0.03675479==\n",
      "===Epoch 3000, the training loss is 0.03621196==\n",
      "===Epoch 4000, the training loss is 0.03544810==\n",
      "===Epoch 5000, the training loss is 0.03500910==\n",
      "===Epoch 6000, the training loss is 0.03394175==\n",
      "===Epoch 7000, the training loss is 0.03077601==\n",
      "===Epoch 8000, the training loss is 0.02979581==\n",
      "===Epoch 9000, the training loss is 0.02446243==\n",
      "===Epoch 10000, the training loss is 0.02407563==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03499913==\n",
      "===Epoch 2000, the training loss is 0.02680185==\n",
      "===Epoch 3000, the training loss is 0.02281443==\n",
      "===Epoch 4000, the training loss is 0.02112615==\n",
      "===Epoch 5000, the training loss is 0.02046081==\n",
      "===Epoch 6000, the training loss is 0.01955608==\n",
      "===Epoch 7000, the training loss is 0.01925872==\n",
      "===Epoch 8000, the training loss is 0.01894058==\n",
      "===Epoch 9000, the training loss is 0.01859666==\n",
      "===Epoch 10000, the training loss is 0.01848061==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03756117==\n",
      "===Epoch 2000, the training loss is 0.03396666==\n",
      "===Epoch 3000, the training loss is 0.03312265==\n",
      "===Epoch 4000, the training loss is 0.03129500==\n",
      "===Epoch 5000, the training loss is 0.03077694==\n",
      "===Epoch 6000, the training loss is 0.03036356==\n",
      "===Epoch 7000, the training loss is 0.03005349==\n",
      "===Epoch 8000, the training loss is 0.02994585==\n",
      "===Epoch 9000, the training loss is 0.02991593==\n",
      "===Epoch 10000, the training loss is 0.02971785==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.04158982==\n",
      "===Epoch 2000, the training loss is 0.03608555==\n",
      "===Epoch 3000, the training loss is 0.03417758==\n",
      "===Epoch 4000, the training loss is 0.03326322==\n",
      "===Epoch 5000, the training loss is 0.03085725==\n",
      "===Epoch 6000, the training loss is 0.02344793==\n",
      "===Epoch 7000, the training loss is 0.02232166==\n",
      "===Epoch 8000, the training loss is 0.02183355==\n",
      "===Epoch 9000, the training loss is 0.02153998==\n",
      "===Epoch 10000, the training loss is 0.01975507==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03841238==\n",
      "===Epoch 2000, the training loss is 0.03702977==\n",
      "===Epoch 3000, the training loss is 0.03571539==\n",
      "===Epoch 4000, the training loss is 0.03432891==\n",
      "===Epoch 5000, the training loss is 0.03420774==\n",
      "===Epoch 6000, the training loss is 0.03378713==\n",
      "===Epoch 7000, the training loss is 0.03328433==\n",
      "===Epoch 8000, the training loss is 0.03309970==\n",
      "===Epoch 9000, the training loss is 0.03293143==\n",
      "===Epoch 10000, the training loss is 0.02915726==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03712411==\n",
      "===Epoch 2000, the training loss is 0.03243759==\n",
      "===Epoch 3000, the training loss is 0.02988598==\n",
      "===Epoch 4000, the training loss is 0.02931256==\n",
      "===Epoch 5000, the training loss is 0.02679757==\n",
      "===Epoch 6000, the training loss is 0.02547795==\n",
      "===Epoch 7000, the training loss is 0.02440591==\n",
      "===Epoch 8000, the training loss is 0.02444523==\n",
      "===Epoch 9000, the training loss is 0.02376624==\n",
      "===Epoch 10000, the training loss is 0.02049534==\n",
      "0\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03960558==\n",
      "===Epoch 2000, the training loss is 0.03565558==\n",
      "===Epoch 3000, the training loss is 0.03328013==\n",
      "===Epoch 4000, the training loss is 0.03286712==\n",
      "===Epoch 5000, the training loss is 0.03265246==\n",
      "===Epoch 6000, the training loss is 0.03223919==\n",
      "===Epoch 7000, the training loss is 0.03191998==\n",
      "===Epoch 8000, the training loss is 0.03181177==\n",
      "===Epoch 9000, the training loss is 0.03175181==\n",
      "===Epoch 10000, the training loss is 0.03160221==\n",
      "1\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03550355==\n",
      "===Epoch 2000, the training loss is 0.03212204==\n",
      "===Epoch 3000, the training loss is 0.03050435==\n",
      "===Epoch 4000, the training loss is 0.02878163==\n",
      "===Epoch 5000, the training loss is 0.02857684==\n",
      "===Epoch 6000, the training loss is 0.02835035==\n",
      "===Epoch 7000, the training loss is 0.02822667==\n",
      "===Epoch 8000, the training loss is 0.02802054==\n",
      "===Epoch 9000, the training loss is 0.02785972==\n",
      "===Epoch 10000, the training loss is 0.02770156==\n",
      "2\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03782523==\n",
      "===Epoch 2000, the training loss is 0.03547065==\n",
      "===Epoch 3000, the training loss is 0.03140631==\n",
      "===Epoch 4000, the training loss is 0.03091845==\n",
      "===Epoch 5000, the training loss is 0.03056764==\n",
      "===Epoch 6000, the training loss is 0.02615958==\n",
      "===Epoch 7000, the training loss is 0.02568008==\n",
      "===Epoch 8000, the training loss is 0.02550106==\n",
      "===Epoch 9000, the training loss is 0.02523082==\n",
      "===Epoch 10000, the training loss is 0.02529760==\n",
      "3\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03517553==\n",
      "===Epoch 2000, the training loss is 0.02979128==\n",
      "===Epoch 3000, the training loss is 0.02903426==\n",
      "===Epoch 4000, the training loss is 0.02786144==\n",
      "===Epoch 5000, the training loss is 0.02741177==\n",
      "===Epoch 6000, the training loss is 0.02731756==\n",
      "===Epoch 7000, the training loss is 0.02727602==\n",
      "===Epoch 8000, the training loss is 0.02704179==\n",
      "===Epoch 9000, the training loss is 0.02673415==\n",
      "===Epoch 10000, the training loss is 0.02663414==\n",
      "4\n",
      "Use L1 Loss\n",
      "===Epoch 1000, the training loss is 0.03815956==\n",
      "===Epoch 2000, the training loss is 0.03391933==\n",
      "===Epoch 3000, the training loss is 0.03261867==\n",
      "===Epoch 4000, the training loss is 0.03183801==\n",
      "===Epoch 5000, the training loss is 0.03155227==\n",
      "===Epoch 6000, the training loss is 0.03152904==\n",
      "===Epoch 7000, the training loss is 0.03133957==\n",
      "===Epoch 8000, the training loss is 0.03115302==\n",
      "===Epoch 9000, the training loss is 0.03090493==\n",
      "===Epoch 10000, the training loss is 0.03044057==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [6:58:46<00:00, 5025.38s/it]  \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### input\n",
    "size = 512\n",
    "l = 1\n",
    "d = defaultdict(list)\n",
    "\n",
    "for i in tqdm(range(5)):\n",
    "    for a in [10, 20, 30, 40]:\n",
    "        \n",
    "        metaload_path = '/mnt/cloud1/sheng-projects/st_projects/spatial_clust/spatial-clust-scripts/ipynb/Bokai_reorg/benchmark/spleen/data/'\n",
    "        df_clean = pd.read_csv('/mnt/cloud1/sheng-projects/st_projects/spatial_clust/spatial-clust-scripts/ipynb/Bokai_reorg/spleen/data/features_and_metadata.csv', index_col=0)\n",
    "        features = np.load('/mnt/cloud1/sheng-projects/st_projects/spatial_clust/spatial-clust-scripts/ipynb/Bokai_reorg/spleen/data/feature_scaled.npy')\n",
    "        \n",
    "        cell_nbhd = np.load(os.path.join(metaload_path,  f\"cell_nbhd_res0.5_k{a}.npy\"))\n",
    "        train_mask = np.load(os.path.join(metaload_path,  \"train_mask.npy\"))\n",
    "        feature_labels = np.load(os.path.join(metaload_path,  \"feature_labels_res0.5.npy\"))\n",
    "        feature_edges = np.load(os.path.join(metaload_path,  \"feature_edges_res0.5.npy\"))\n",
    "        spatial_edges = np.load(os.path.join(metaload_path,  \"spatial_edges_0326.npy\"))                       \n",
    "                               \n",
    "        # change into torch\n",
    "        features = torch.from_numpy(features).float().to(args.device)\n",
    "        feat_edge_index = torch.from_numpy(np.array(feature_edges.T[:2])).long().to(args.device)\n",
    "        spat_edge_index = torch.from_numpy(np.array(spatial_edges.T[:2])).long().to(args.device)\n",
    "        \n",
    "        # combo nbhd                       \n",
    "        df_clean['res'] = feature_labels\n",
    "        reslabel = pd.get_dummies(df_clean['res'])\n",
    "        combo_nbhd = np.hstack([reslabel, cell_nbhd])\n",
    "        combo_nbhd = torch.from_numpy(combo_nbhd).float().to(args.device)\n",
    "        \n",
    "        ## cnn\n",
    "        load_path = '/mnt/cloud1/sheng-projects/st_projects/spatial_clust/spatial-clust-scripts/ipynb/Bokai_reorg/benchmark/spleen/data/'\n",
    "        save_folder = os.path.join(load_path, \"cnn\", f\"cnn_512_l{l}_layer6_testnbsize:{a}_checkpoints\", \"epochs\", 'embed')\n",
    "        args.out_dim = combo_nbhd.shape[1]\n",
    "        \n",
    "        #### reset args here\n",
    "        class Args:\n",
    "            gnn_input_dim = 31\n",
    "            cnn_input_dim = 128\n",
    "            fc_dim = latent_dim = 32\n",
    "            cnn_dim = cnn_latent_dim = 32\n",
    "            out_dim = combo_nbhd.shape[1]\n",
    "\n",
    "            fc_out_dim = 33\n",
    "            cnn_out_dim = 11\n",
    "            hid_out_dim = 33\n",
    "\n",
    "            criterion = \"L1\"\n",
    "            learning_rate = 1e-3\n",
    "            epochs = 10000\n",
    "            print_every = 1000\n",
    "            average_iter = 100\n",
    "            device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        args = Args()\n",
    "        #### reseat args finished\n",
    "        \n",
    "        # get cnn embedding\n",
    "        epoch = 400\n",
    "        cnn_embedding = np.load(os.path.join(save_folder, f'cnn_512_testnbsize:{a}_l1_layer6_byepoch' ,f\"cnn_embedding_512_full_l1_dim128_epoch{epoch}.npy\"))\n",
    "        cnn_embedding = torch.from_numpy(cnn_embedding).float().to(args.device)\n",
    "        cnn = cnn_embedding\n",
    "        \n",
    "        stable = True\n",
    "        if stable:\n",
    "            rep = 5\n",
    "            dim = args.fc_out_dim + args.cnn_out_dim\n",
    "            concat_embedding = np.zeros((features.shape[0], rep * dim))\n",
    "            for i in range(rep):\n",
    "                print(i)\n",
    "                gnn_embedding = get_gnn_embed(SNAP_GNN(args), combo_nbhd, features, cnn, feat_edge_index, spat_edge_index, verbose=True)\n",
    "                concat_embedding[:, i*dim : (i+1)*dim] = gnn_embedding\n",
    "            Ue, Se, Vhe = np.linalg.svd(concat_embedding, full_matrices=False)\n",
    "\n",
    "            plt.plot(Se)\n",
    "            k = 32\n",
    "            gnn_embedding = Ue[:, :k] @ np.diag(Se[:k])\n",
    "        else:\n",
    "            gnn_embedding = get_gnn_embed(SNAP_GNN(args), combo_nbhd, features, cnn, feat_edge_index, spat_edge_index, verbose=True)\n",
    "\n",
    "        ## save out\n",
    "        dir = '../data/saved_embedding/nbhd_k' + str(a) + '_dbGNN.npy'\n",
    "        np.save(dir, gnn_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellsnap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
