{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db694681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PhenoGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc71c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bokai/miniconda3/envs/muse/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import muse_sc as muse\n",
    "import phenograph\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da44837f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bokai/miniconda3/envs/muse/bin/python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56fadaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "###### process protein modality\n",
    "\n",
    "data1 = pd.read_csv(\"../data/livcan_counts_y4_x69.csv\")\n",
    "dataa = data1\n",
    "\n",
    "## change to scaled version\n",
    "from scipy.stats import zscore\n",
    "dataa = dataa.apply(zscore)\n",
    "dataa = dataa\n",
    "\n",
    "####### process imaging modality\n",
    "\n",
    "datab_full = np.load('../data/single_cell_images/Inception_img_feature_liver_469.npy')\n",
    "\n",
    "###### this is pca on feature mod and image mod\n",
    "\n",
    "latent_dim = 20\n",
    "view_a_feature = PCA(n_components=latent_dim).fit_transform(dataa)\n",
    "### this is pca on img mod\n",
    "latent_dim = 100\n",
    "view_b_feature = PCA(n_components=latent_dim).fit_transform(datab_full)\n",
    "### then default clustering\n",
    "\n",
    "# already finished running\n",
    "view_a_label, _, _ = phenograph.cluster(view_a_feature, n_jobs = 16)\n",
    "view_b_label, _, _ = phenograph.cluster(view_b_feature, n_jobs = 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99de16ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../data/muse469_a_label.npy', view_a_label)\n",
    "np.save('../data/muse469_b_label.npy', view_b_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f20728a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_a_label = np.load('../data/muse469_a_label.npy')\n",
    "view_b_label = np.load('../data/muse469_b_label.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa5d3b",
   "metadata": {},
   "source": [
    "### change original muse function phenograph cpu allocation potential bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "376f5a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from muse_sc.muse_architecture import structured_embedding\n",
    "from scipy.spatial.distance import pdist\n",
    "import phenograph\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "def muse_fit_predict(data_x,\n",
    "                     data_y,\n",
    "                     label_x,\n",
    "                     label_y,\n",
    "                     latent_dim=100,\n",
    "                     n_epochs=500,\n",
    "                     lambda_regul=5,\n",
    "                     lambda_super=5):\n",
    "    \"\"\"\n",
    "        MUSE model fitting and predicting:\n",
    "          This function is used to train the MUSE model on multi-modality data\n",
    "\n",
    "        Parameters:\n",
    "          data_x:       input for transcript modality; matrix of  n * p, where n = number of cells, p = number of genes.\n",
    "          data_y:       input for morphological modality; matrix of n * q, where n = number of cells, q is the feature dimension.\n",
    "          label_x:      initial reference cluster label for transcriptional modality.\n",
    "          label_y:      inital reference cluster label for morphological modality.\n",
    "          latent_dim:   feature dimension of joint latent representation.\n",
    "          n_epochs:     maximal epoch used in training.\n",
    "          lambda_regul: weight for regularization term in the loss function.\n",
    "          lambda_super: weight for supervised learning loss in the loss function.\n",
    "\n",
    "        Output:\n",
    "          latent:       joint latent representation learned by MUSE.\n",
    "          reconstruct_x:reconstructed feature matrix corresponding to input data_x.\n",
    "          reconstruct_y:reconstructed feature matrix corresponding to input data_y.\n",
    "          latent_x:     modality-specific latent representation corresponding to data_x.\n",
    "          latent_y:     modality-specific latent representation corresponding to data_y.\n",
    "\n",
    "        Feng Bao @ Altschuler & Wu Lab @ UCSF 2022.\n",
    "        Software provided as is under MIT License.\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" initial parameter setting \"\"\"\n",
    "    # parameter setting for neural network\n",
    "    n_hidden = 128  # number of hidden node in neural network\n",
    "    learn_rate = 1e-4  # learning rate in the optimization\n",
    "    batch_size = 64  # number of cells in the training batch\n",
    "    n_epochs_init = 200  # number of training epoch in model initialization\n",
    "    print_epochs = 50  # epoch interval to display the current training loss\n",
    "    cluster_update_epoch = 200  # epoch interval to update modality-specific clusters\n",
    "\n",
    "    # read data-specific parameters from inputs\n",
    "    feature_dim_x = data_x.shape[1]\n",
    "    feature_dim_y = data_y.shape[1]\n",
    "    n_sample = data_x.shape[0]\n",
    "\n",
    "    # GPU configuration\n",
    "    # config = tf.ConfigProto()\n",
    "    # config.gpu_options.allow_growth = True\n",
    "\n",
    "    \"\"\" construct computation graph using TensorFlow \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # raw data from two modalities\n",
    "    x = tf.placeholder(tf.float32, shape=[None, feature_dim_x], name='input_x')\n",
    "    y = tf.placeholder(tf.float32, shape=[None, feature_dim_y], name='input_y')\n",
    "\n",
    "    # labels inputted for references\n",
    "    ref_label_x = tf.placeholder(tf.float32, shape=[None], name='ref_label_x')\n",
    "    ref_label_y = tf.placeholder(tf.float32, shape=[None], name='ref_label_y')\n",
    "\n",
    "    # hyperparameter in triplet loss\n",
    "    triplet_lambda = tf.placeholder(tf.float32, name='triplet_lambda')\n",
    "    triplet_margin = tf.placeholder(tf.float32, name='triplet_margin')\n",
    "\n",
    "    # network architecture\n",
    "    z, x_hat, y_hat, encode_x, encode_y, loss, \\\n",
    "    reconstruction_error, weight_penalty, \\\n",
    "    trip_loss_x, trip_loss_y = structured_embedding(x,\n",
    "                                                    y,\n",
    "                                                    ref_label_x,\n",
    "                                                    ref_label_y,\n",
    "                                                    latent_dim,\n",
    "                                                    triplet_margin,\n",
    "                                                    n_hidden,\n",
    "                                                    lambda_regul,\n",
    "                                                    triplet_lambda)\n",
    "    # optimization operator\n",
    "    train_op = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
    "    print('++++++++++ MUSE for multi-modality single-cell analysis ++++++++++')\n",
    "    \"\"\" MUSE optimization \"\"\"\n",
    "    total_batch = int(n_sample / batch_size)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        \"\"\" initialization of autoencoder architecture for MUSE \"\"\"\n",
    "        print('MUSE initialization')\n",
    "        # global parameter initialization\n",
    "        sess.run(tf.global_variables_initializer(), feed_dict={triplet_lambda: 0,\n",
    "                                                               triplet_margin: 0})\n",
    "\n",
    "        for epoch in range(n_epochs_init):\n",
    "            # randomly permute samples\n",
    "            random_idx = np.random.permutation(n_sample)\n",
    "            data_train_x = data_x[random_idx, :]\n",
    "            data_train_y = data_y[random_idx, :]\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                # input data batches\n",
    "                offset = (i * batch_size) % (n_sample)\n",
    "                batch_x_input = data_train_x[offset:(offset + batch_size), :]\n",
    "                batch_y_input = data_train_y[offset:(offset + batch_size), :]\n",
    "\n",
    "                # initialize parameters without self-supervised loss (triplet_lambda=0)\n",
    "                sess.run(train_op,\n",
    "                         feed_dict={x: batch_x_input,\n",
    "                                    y: batch_y_input,\n",
    "                                    ref_label_x: np.zeros(batch_x_input.shape[0]),\n",
    "                                    ref_label_y: np.zeros(batch_y_input.shape[0]),\n",
    "                                    triplet_lambda: 0,\n",
    "                                    triplet_margin: 0})\n",
    "\n",
    "            # calculate and print loss terms for current epoch\n",
    "            if epoch % print_epochs == 0:\n",
    "                L_total, L_reconstruction, L_weight = \\\n",
    "                    sess.run((loss, reconstruction_error, weight_penalty),\n",
    "                             feed_dict={x: data_train_x,\n",
    "                                        y: data_train_y,\n",
    "                                        ref_label_x: np.zeros(data_train_x.shape[0]),  # no use as triplet_lambda=0\n",
    "                                        ref_label_y: np.zeros(data_train_y.shape[0]),  # no use as triplet_lambda=0\n",
    "                                        triplet_lambda: 0,\n",
    "                                        triplet_margin: 0})\n",
    "\n",
    "                print(\n",
    "                    \"epoch: %d, \\t total loss: %03.5f,\\t reconstruction loss: %03.5f,\\t sparse penalty: %03.5f\"\n",
    "                    % (epoch, L_total, L_reconstruction, L_weight))\n",
    "\n",
    "        # estimate the margin for the triplet loss\n",
    "        latent, reconstruct_x, reconstruct_y = \\\n",
    "            sess.run((z, x_hat, y_hat),\n",
    "                     feed_dict={x: data_x,\n",
    "                                y: data_y,\n",
    "                                ref_label_x: np.zeros(data_x.shape[0]),\n",
    "                                ref_label_y: np.zeros(data_y.shape[0]),\n",
    "                                triplet_lambda: 0,\n",
    "                                triplet_margin: 0})\n",
    "        latent_pd_matrix = pdist(latent, 'euclidean')\n",
    "        latent_pd_sort = np.sort(latent_pd_matrix)\n",
    "        select_top_n = np.int(latent_pd_sort.size * 0.2)\n",
    "        margin_estimate = np.median(latent_pd_sort[-select_top_n:]) - np.median(latent_pd_sort[:select_top_n])\n",
    "\n",
    "        # refine MUSE parameters with reference labels and triplet losses\n",
    "        for epoch in range(n_epochs_init):\n",
    "            # randomly permute samples\n",
    "            random_idx = np.random.permutation(n_sample)\n",
    "            data_train_x = data_x[random_idx, :]\n",
    "            data_train_y = data_y[random_idx, :]\n",
    "            label_train_x = label_x[random_idx]\n",
    "            label_train_y = label_y[random_idx]\n",
    "\n",
    "            for i in range(total_batch):\n",
    "                # data batches\n",
    "                offset = (i * batch_size) % (n_sample)\n",
    "                batch_x_input = data_train_x[offset:(offset + batch_size), :]\n",
    "                batch_y_input = data_train_y[offset:(offset + batch_size), :]\n",
    "                label_x_input = label_train_x[offset:(offset + batch_size)]\n",
    "                label_y_input = label_train_y[offset:(offset + batch_size)]\n",
    "\n",
    "                # refine parameters\n",
    "                sess.run(train_op,\n",
    "                         feed_dict={x: batch_x_input,\n",
    "                                    y: batch_y_input,\n",
    "                                    ref_label_x: label_x_input,\n",
    "                                    ref_label_y: label_y_input,\n",
    "                                    triplet_lambda: lambda_super,\n",
    "                                    triplet_margin: margin_estimate})\n",
    "\n",
    "            # calculate loss on all input data for current epoch\n",
    "            if epoch % print_epochs == 0:\n",
    "                L_total, L_reconstruction, L_weight, L_trip_x, L_trip_y = \\\n",
    "                    sess.run((loss, reconstruction_error, weight_penalty, trip_loss_x, trip_loss_y),\n",
    "                             feed_dict={x: data_train_x,\n",
    "                                        y: data_train_y,\n",
    "                                        ref_label_x: label_train_x,\n",
    "                                        ref_label_y: label_train_y,\n",
    "                                        triplet_lambda: lambda_super,\n",
    "                                        triplet_margin: margin_estimate})\n",
    "\n",
    "                print(\n",
    "                    \"epoch: %d, \\t total loss: %03.5f,\\t reconstruction loss: %03.5f,\\t sparse penalty: %03.5f,\\t x triplet: %03.5f,\\t y triplet: %03.5f\"\n",
    "                    % (epoch, L_total, L_reconstruction, L_weight, L_trip_x, L_trip_y))\n",
    "\n",
    "        # update cluster labels based modality-specific latents\n",
    "        latent_x, latent_y = \\\n",
    "            sess.run((encode_x, encode_y),\n",
    "                     feed_dict={x: data_x,\n",
    "                                y: data_y,\n",
    "                                ref_label_x: label_x,\n",
    "                                ref_label_y: label_y,\n",
    "                                triplet_lambda: lambda_super,\n",
    "                                triplet_margin: margin_estimate})\n",
    "\n",
    "        # update cluster labels using PhenoGraph\n",
    "        label_x_update, _, _ = phenograph.cluster(latent_x, n_jobs= 2)\n",
    "        label_y_update, _, _ = phenograph.cluster(latent_y, n_jobs= 2)\n",
    "        print('Finish initialization of MUSE')\n",
    "\n",
    "        ''' Training of MUSE '''\n",
    "        for epoch in range(n_epochs):\n",
    "            # randomly permute samples\n",
    "            random_idx = np.random.permutation(n_sample)\n",
    "            data_train_x = data_x[random_idx, :]\n",
    "            data_train_y = data_y[random_idx, :]\n",
    "            label_train_x = label_x_update[random_idx]\n",
    "            label_train_y = label_y_update[random_idx]\n",
    "\n",
    "            # loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                # batch data\n",
    "                offset = (i * batch_size) % (n_sample)\n",
    "                batch_x_input = data_train_x[offset:(offset + batch_size), :]\n",
    "                batch_y_input = data_train_y[offset:(offset + batch_size), :]\n",
    "                batch_label_x_input = label_train_x[offset:(offset + batch_size)]\n",
    "                batch_label_y_input = label_train_y[offset:(offset + batch_size)]\n",
    "\n",
    "                sess.run(train_op,\n",
    "                         feed_dict={x: batch_x_input,\n",
    "                                    y: batch_y_input,\n",
    "                                    ref_label_x: batch_label_x_input,\n",
    "                                    ref_label_y: batch_label_y_input,\n",
    "                                    triplet_lambda: lambda_super,\n",
    "                                    triplet_margin: margin_estimate})\n",
    "\n",
    "            # calculate and print losses on whole training dataset\n",
    "            if epoch % print_epochs == 0:\n",
    "                L_total, L_reconstruction, L_weight, L_trip_x, L_trip_y = \\\n",
    "                    sess.run((loss, reconstruction_error, weight_penalty, trip_loss_x, trip_loss_y),\n",
    "                             feed_dict={x: data_train_x,\n",
    "                                        y: data_train_y,\n",
    "                                        ref_label_x: label_train_x,\n",
    "                                        ref_label_y: label_train_y,\n",
    "                                        triplet_lambda: lambda_super,\n",
    "                                        triplet_margin: margin_estimate})\n",
    "                # print cost every epoch\n",
    "                print(\n",
    "                    \"epoch: %d, \\t total loss: %03.5f,\\t reconstruction loss: %03.5f,\\t sparse penalty: %03.5f,\\t x triplet loss: %03.5f,\\t y triplet loss: %03.5f\"\n",
    "                    % (epoch, L_total, L_reconstruction, L_weight, L_trip_x, L_trip_y))\n",
    "\n",
    "            # update cluster labels based on new modality-specific latent representations\n",
    "            if epoch % cluster_update_epoch == 0:\n",
    "                latent_x, latent_y = \\\n",
    "                    sess.run((encode_x, encode_y),\n",
    "                             feed_dict={x: data_x,\n",
    "                                        y: data_y,\n",
    "                                        ref_label_x: label_x,\n",
    "                                        ref_label_y: label_y,\n",
    "                                        triplet_lambda: lambda_super,\n",
    "                                        triplet_margin: margin_estimate})\n",
    "\n",
    "                # use PhenoGraph to obtain cluster label\n",
    "                label_x_update, _, _ = phenograph.cluster(latent_x, n_jobs= 2)\n",
    "                label_y_update, _, _ = phenograph.cluster(latent_y, n_jobs= 2)\n",
    "\n",
    "        \"\"\" MUSE output \"\"\"\n",
    "        latent, reconstruct_x, reconstruct_y, latent_x, latent_y = \\\n",
    "            sess.run((z, x_hat, y_hat, encode_x, encode_y),\n",
    "                     feed_dict={x: data_x,\n",
    "                                y: data_y,\n",
    "                                ref_label_x: label_x,  # no effects to representations\n",
    "                                ref_label_y: label_y,  # no effects to representations\n",
    "                                triplet_lambda: lambda_super,\n",
    "                                triplet_margin: margin_estimate})\n",
    "\n",
    "        print('++++++++++ MUSE completed ++++++++++')\n",
    "\n",
    "    return latent, reconstruct_x, reconstruct_y, latent_x, latent_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deeb76a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## this is just for testing\n",
    "\n",
    "################################# loop batch version, random sample\n",
    "for i in range(5):\n",
    "\n",
    "  np.random.seed(i)\n",
    "  indices = np.random.choice(datab_full.shape[0], 10000, replace=False)\n",
    "  dataa_sub = dataa.iloc[indices,:]\n",
    "  datab_sub = datab_full[indices,:]\n",
    "  view_a_label_sub = view_a_label[indices]\n",
    "  view_b_label_sub = view_b_label[indices]\n",
    "\n",
    "  muse_feature, reconstruct_x, reconstruct_y, \\\n",
    "    latent_x, latent_y = muse_fit_predict(dataa_sub.to_numpy(),\n",
    "                                          datab_sub,\n",
    "                                          view_a_label_sub,\n",
    "                                          view_b_label_sub,\n",
    "                                          latent_dim=30,\n",
    "                                          n_epochs=500,\n",
    "                                          lambda_regul=5,\n",
    "                                          lambda_super=5)\n",
    "  name = 'batch_' + str(i) + '.npy'\n",
    "  np.save('../data/muse_469_embedding_' + name, muse_feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
